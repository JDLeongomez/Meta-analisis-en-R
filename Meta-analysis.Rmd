---
title: 'Meta-análisis de correlaciones y meta-regresión en R:'
subtitle: 'Guía práctica'
author:
  - name: Juan David Leongómez \orcidlink{0000-0002-0092-6298}
    correspondence: false
date: "`r Sys.setlocale('LC_TIME');format(Sys.Date(),'%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    citation_package: biblatex
    number_sections: yes
    keep_tex:  true
    toc: no
    pandoc_args:
      - '--lua-filter=lua/scholarly-metadata.lua'
      - '--lua-filter=lua/author-info-blocks.lua'
      - '--highlight-style=theme/my_style.theme'
classoption: 
      - bookmarksnumbered
editor_options:
  chunk_output_type: console
geometry: margin=2cm
header-includes: 
  \usepackage{setspace}
  \usepackage{float} 
  \floatplacement{figure}{H}
  \usepackage[T1]{fontenc} 
  \usepackage[utf8]{inputenc}
  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \lhead{Juan David Leongómez}
  \rhead{\textit{Meta-análisis de correlaciones en {R:} Guía práctica}}
  \rfoot{\footnotesize{{doi:}
    \href{https://doi.org/10.31222/osf.io/yaxd4}{10.31222/osf.io/yaxd4}}}
  \lfoot{\footnotesize{Versión 2}}
  \renewcommand{\abstractname}{Descripción}
  \usepackage[spanish]{babel}
  \usepackage{csquotes}
  \usepackage[style=apa,backend=biber]{biblatex}
  \DeclareLanguageMapping{spanish}{spanish-apa}
  \usepackage{hanging}
  \usepackage{amsthm,amssymb,amsfonts}
  \usepackage{tikz,lipsum,lmodern}
  \usepackage[most]{tcolorbox}
  \usepackage{multicol}
  \usepackage{fontawesome5}
  \usepackage{multirow,booktabs,caption}
  \renewcommand\spanishtablename{Tabla}
  \DeclareCaptionLabelSeparator*{spaced}{\\[1ex]}
  \DeclareCaptionLabelSeparator{point}{. }
  \captionsetup[table]{labelfont=bf,
    textfont=it,
    format=plain,
    justification=justified,
    singlelinecheck=false,
    labelsep=spaced,
    skip=5pt}
  \captionsetup[figure]{labelfont=bf,
    format=plain,
    justification=justified,
    singlelinecheck=false,labelsep=point,skip=5pt}
  \captionsetup[figure]{font=small}
  \usepackage{orcidlink}
  \definecolor{iacol}{RGB}{246, 130, 18}
  \definecolor{iacoldark}{RGB}{246, 100, 18}
  \newcommand{\opensupplement}{\setcounter{table}{0}
    \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0}
    \renewcommand{\thefigure}{A\arabic{figure}}}
  \newcommand{\closesupplement}{\setcounter{table}{0}
    \renewcommand{\thetable}{\arabic{table}} \setcounter{figure}{0}
    \renewcommand{\thefigure}{\arabic{figure}}}
always_allow_html: yes
csl: apa.csl
urlcolor: blue
linkcolor: iacoldark
citecolor: iacoldark
link-citations: true
bibliography: bib/references.bib
---

```{=latex}
\newtcolorbox[auto counter]{ROut}[2][]{
                lower separated=false,
                colback=white,
                colframe=iacol,
                fonttitle=\bfseries,
                colbacktitle=iacol,
                coltitle=black,
                boxrule=1pt,
                sharp corners,
                breakable,
                enhanced,
                attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
                boxed title style={boxrule=0pt,colframe=white,},
              title=#2,#1}
```

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(comment = NA)
def_hook <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options){
  out <- def_hook(x, options)
  return(paste("\\begin{ROut}{Consola de R: Output~\\thetcbcounter}
                \\begin{footnotesize}
                \\begin{verbatim}", 
               x,
               "\\end{verbatim}
                \\end{footnotesize}
                \\end{ROut}"))
})
library(robumeta)
library(metafor)
library(tidyverse)
library(ggpubr)
library(kableExtra)
library(scales)
```

```{=latex}
\begin{center}
\textit{\textbf{EvoCo}: Laboratorio de Evolución y Comportamiento Humano}, Facultad de Psicología, Universidad El Bosque, Bogotá, Colombia. Email: \href{mailto:jleongomez@unbosque.edu.co}{jleongomez@unbosque.edu.co}. Web: \href{https://jdleongomez.info/es}{jdleongomez.info}.

\vfill
\textbf{Resumen}
\end{center}

\par
\begingroup
\leftskip3em
\rightskip\leftskip
```

El meta-análisis es un método ampliamente utilizado para sintetizar los datos de diferentes estudios. Sin embargo estudiantes, profesionales e investigadores a menudo carecemos de conocimientos prácticos para hacer e interpretar un meta-análisis. Esta guía presenta una variedad de herramientas para realizar meta-análisis de correlaciones en R, mediante el uso de ejemplos reales. Incluye desde análisis simples y su interpretación, hasta el análisis de moderadores (meta-regresión), usando los paquetes [`metafor`](https://www.metafor-project.org/doku.php) [@viechtbauer2010] y [`metaviz`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html) [@KossmeierMetaviz]. También incluye explicaciones para la transformación de coeficientes *r* de Pearson a *z* de Fisher (y viceversa), creación de gráficos de bosque (*forest plots*) y gráficos de embudo (*funnel plots*), análisis de heterogeneidad y diagnósticos de influencia, así como estrategias para detectar posibles sesgos de publicación utilizando el paquete [`weightr`](https://www.r-pkg.org/pkg/weightr) [@coburnWeightr2019], y para determinar el poder estadístico de un meta-análisis utilizando [`metameta`](https://www.dsquintana.blog/metameta-r-package-meta-analysis/) [@quintanaMetameta2022]. Con esta guía, los lectores podrán adquirir las habilidades necesarias para realizar meta-análisis de correlaciones de manera efectiva en R y obtener resultados confiables.

```{=latex}
\par
\endgroup

\par
\begingroup
\leftskip3em
\rightskip\leftskip
\footnotesize
```

[![Investigación Abierta](images%5CLogo-IA-Rectangulo.pdf){width=600px}](https://www.youtube.com/@InvestigacionAbierta)

**Conocimientos necesarios:** Esta guía asume (1) un manejo básico de R, (2) comprensión de correlaciones y regresiones, y (3) un entendimiento de los principios del meta-análisis. De ser necesario, para adquirir la comprensión básica y necesaria del meta-análisis, recomiendo ver [este video](https://youtu.be/ntBbkOn9D_o) introductorio al meta-análisis en *jamovi* [@leongomezMetaanalysis2021] publicado en mi canal de YouTube [*Investigación Abierta*](https://www.youtube.com/@InvestigacionAbierta).

**Versión de R:** Para poder correr el código tal cual aparece en esta guía, en se requiere R versión 4.1 o superior, pues he usado el *pipe* nativo de R (`|>`). En versiones anteriores de R, el *pipe* nativo puede ser reemplazado por el pipe de `magrittr` (`%>%`), cargando ese paquete [@magrittrcite]. Para más información, puedes ver la sección [11.1.5](https://datanalytics.com/libro_r/1100_programacion.html#tuber%C3%ADas-pipes) de @gil2023.

```{=latex}
\par
\endgroup
\vfill

\textbf{Cita esta guía como } \hrulefill 

\begin{hangparas}{.25in}{1}
Leongómez, J. D. (2022). Meta-análisis de correlaciones y meta-regresión en R: Guía práctica. \textit{MetaArXiv}. \url{https://doi.org/10.31222/osf.io/yaxd4}
\end{hangparas}
\newpage

{\hypersetup{hidelinks}
 \setcounter{tocdepth}{5}
 \tableofcontents
}
```

# Introducción

En investigación, es frecuente que busquemos medir la magnitud de un efecto en una población. Por ejemplo, podríamos tener interés en determinar cosas tan diversas como qué tan fuerte es la correlación entre la masa de una estrella y su luminosidad, o entre el nivel socioeconómico y el índice de masa corporal (IMC) en un país determinado. No obstante, es muy poco probable que podamos obtener información sobre la población entera (de estrellas en el universo o de personas del país), por lo que debemos hacer uso de muestras de esa población para inferir la magnitud del efecto. 

Sin embargo, a menudo, diferentes estudios obtienen coeficientes de correlación ligeramente diferentes (en algunos casos, muy diferentes). Esta variabilidad no es sorprendente, ya que cada muestra es solo una visualización parcial (y de alguna manera sesgada) de la correlación en la población entera. 

Por ejemplo mira la simulación a continuación (Fig. \@ref(fig:efecto)). En el panel **A** he simulado una población con un *N* de 6095 elementos (por ejemplo, personas, estrellas, o lo que sea relevante para ti). En esa población hipotética, la correlación entre dos variables es de $r$ = 0.25. 

```{r efecto, echo = FALSE, fig.cap = "Simulación de una población hipotética de *N* = 6095 elementos, en la que dos variables se correlacionan positivamente (*r* = 0.25). **A.** Simulación de la población y correlación entre dos variables. **B.** 25 muestras aleatorias de diferentes tamaños (*n*) de esa población, y la correlación encontrada en cada una (*r*). En naranja se resaltan los elementos seleccionados aleatoriamente y la correlación encontrada en cada muestra, superpuestos sobre el total de la población y la correlación real (en negro, como referencia)."}
knitr::include_graphics("images/sim_estabilidad.pdf")
```

Ahora imagina que tú quieres establecer esa correlación real ($r$ = 0.25), pero no la conoces, y no tienes acceso a la población completa de 6095 elementos. Tu mejor apuesta es tomar una muestra aleatoria, que en principio tendería a darte un resultado *similar* al real. En el panel **B** hay 25 muestras aleatorias de esos 6095 elementos, y en cada una se reporta tanto el tamaño de la muestra (*n*, que va desde una muestra muy pequeña de apenas 14 observaciones, hasta una muy respetable de 493), como la correlación encontrada en esa muestra (*r*, que va desde 0.10 hasta 0.46). 

Entre las 25 muestras aleatorias de nuestra población (Fig. \@ref(fig:efecto)B), hay varias que encuentran una correlación similar a la real (que, como es simulada, sabemos que es exactamente de $r$ = 0.25), pero hay otras que se alejan bastante. Analiza detalladamente esos resultados y trata de ver si las que más se alejan (estando por debajo o por encima de 0.25), tienen algo en común.

Si miras con cuidado, verás algo claro: las muestras más grandes tienden a encontrar resultados similares al de nuestra población simulada ($r$ = 0.25). Sin embargo, las muestras pequeñas son menos confiables: aunque algunas muestras pequeñas encuentran correlaciones muy cercanas a la real, otras se alejan mucho. Para ilustrar esto, voy a graficar la correlación encontrada en cada muestra, y cómo se asocia con su tamaño de muestra (Fig. \@ref(fig:dist-sim)).

```{r dist-sim, echo = FALSE, fig.align = "center", fig.cap = "Relación entre el efecto encontrado (eje $X$) y el tamaño de muestra (eje $Y$) en 25 muestras aleatorias de una población simulada donde la correlación entre las variables es de $r$ = 0.25. Mientras que las muestras grandes (arriba) son confiables y tienden a encontrar resultados similares al del efeto real, muestras pequeñas (abajo) son menos confiables y pueden producir estimaciones del efecto que se alejan mucho del real."}
knitr::include_graphics("images/dist-sim.pdf")
```

## ¿Qué es un meta-análisis y por qué es importante?

De manera más general, se puede entender así: aunque hay técnicas de muestreo que tienden a hacer que las muestras se *parezcan* más a la población, cada muestra tiene características particulares que conducen a resultados diferentes. Entonces, cuando debemos hacer inferencia con base en muestras porque no podemos tener acceso a la población entera, ¿cómo podemos saber cuál es el efecto real si cada estudio obtiene un resultado distinto?

Una herramienta útil para abordar este problema es el meta-análisis, que es una técnica estadística que nos permite obtener una estimación más precisa del efecto *real* de la correlación. Por supuesto, la palabra clave acá es *estimación*, pues aunque el efecto real es algo que rara vez podremos determinar con absoluta certeza, sí es algo que podemos acercarnos a conocer. 

El meta-análisis se puede entender como una especie *promediación* de los efectos encontrados en diversos estudios, dando mayor peso a estudios que tienen más credibilidad (es decir, mayor probabilidad de mostrar un resultado cercano a la verdad). ¿Cómo se hace esto? Como se ve en la Figura \@ref(fig:dist-sim), el tamaño de una muestra afecta la confiabilidad de un resultado. 

Para decirlo de manera más técnica, el tamaño de la muestra afecta el poder estadístico así como el error en la estimación del efecto [@leongomezPoderRvid2020; @leongomezAnalisisPoderEstadistico2020]; entre mayor sea una muestra, mayor será el poder estadístico y menor tenderá a ser su variación con respecto al efecto real (en últimas, muestras más grandes tienden a parecerse más a la población de la que provienen y por tanto son más confiables). Por esto, podemos usar estas medidas como indicadores de la credibilidad del efecto encontrado por un estudio.

## Sesgos de publicación

Sin embargo, además de las diferencias que pueda tener cada estudio debido a las peculiaridades y tamaño de su muestra, con frecuencia al hacer meta-análisis nos enfrentamos a un riesgo más sutil y menos predecible: los sesgos de publicación.

Los sesgos de publicación son una amenaza importante para la integridad de la investigación científica. Se refieren a la tendencia de los investigadores y editores de revistas a no publicar estudios que no encuentran un efecto significativo o que muestran resultados contrarios a las hipótesis de sus autores. Estos sesgos pueden afectar a todas las áreas de la investigación, pasando por ejemplo por la medicina, la psicología y la biología [e.g., @baker500ScientistsLift2016; @kleinManyLabsInvestigating2018]. 

Como resultado, los estudios publicados pueden no ser representativos de la verdadera relación entre las variables que se están investigando, lo que puede llevar a conclusiones erróneas y a decisiones equivocadas, incluyendo áreas tan críticas como la práctica clínica y la toma de decisiones políticas, entre otras.

Imagina, por ejemplo, que frente a un efecto estudiado se publicaron preferentemente estudios que encontraron una correlación positiva y significativa, mientras que otros estudios —que no encontraron una correlación o encontraron una correlación negativa—  tendieron a no ser publicados por los autores, o a ser rechazados por editores. Al hacer un meta-análisis de los estudios publicados, nuestra estimación del efecto *real* estaría sesgada, sobre-estimando el efecto (Fig. \@ref(fig:teoria-meta)).

```{r teoria-meta, echo = FALSE, fig.cap = "Ejemplo hipotético de distribución de resultados de diferentes estudios (puntos grises) en relación a un efecto real (línea gris vertical). Los estudios realizados con diferentes muestras tienden a estar a ambos lados del efecto real (efectos mayores al real a la derecha y efectos menores a la izquierda). Sin embargo, entre mayor sea la muestra (arriba) los estudios tienden a estar más cerca del efecto real (tienen menor error), pero estudios con muestras pequeñas (abajo) son más variables (y en promedio tienen mayor error), y pueden tener efectos muy alejados del efecto real. Esto forma una especie de triángulo simétrico, con el efecto real en el centro. Esta idea, aunque simplificada en este ejemplo, está en la base misma del meta-análisis, y es similar a lo que comúnmente se representa en un diagrama de embudo (\\textit{funnel plot}). \\textbf{A.} Distribución cuando no hay sesgo de publicación. \\textbf{B.} Distribución de resultados en el caso hipotético de que algunos estudios con efectos bajos no fueran publicados (puntos naraja claros). Debido a esto, un meta-análisis estaría sesgado, y el efecto estimado (línea gris punteada) sería mayor al efecto real (línea gris sólida)."}
knitr::include_graphics("images/teoria-meta.pdf")
```

Por suerte, existen técnicas poderosas para buscar evidencia de sesgos de publicación en los estudios incluidos en un meta-análisis, y hacer estimaciones corregidas del efecto *real* que tengan estos sesgos en cuenta para hacer estimaciones más precisas.

## Motivación

Teniendo en cuenta la importancia de estas técnicas, y la relativa escasez de material de buena calidad en español, accesible tanto para estudiantes como para investigadores establecidos, he decidido hacer esta guía. He incluido información actualizada, soportada siempre en fuentes de alta calidad que pueden ser consultadas por las personas que quieran profundizar en cada tema.

A lo largo del documento daré ejemplos e instrucciones precisas de cómo hacer un meta-análisis básico en R (sección \@ref(meta-cor)), incluyendo cómo hacer análisis de heterogeneidad (sección \@ref(heterog-inf)) y diagnósticos de influencia (sección \@ref(diag-inf)), creación de gráficos de bosque (*forest plots*; sección \@ref(forest-inf)) y gráficos de embudo (*funnel plots*; sección \@ref(funnel-inf)), así como estrategias para detectar posibles sesgos de publicación (secciones \@ref(sesgo-pub) y \@ref(todas-combinaciones)), y para determinar el poder estadístico de un meta-análisis (sección \@ref(poder-inf)). Finalmente, mostraré técnicas de meta-regresión para investigar los factores que pueden influir en los resultados de los estudios incluidos en un meta-análisis (sección \@ref(met-moderation)). 

Adicionalmente, he incluido algunos apéndices, donde encontrarás información concreta sobre otras rutas alternativas para hacer meta-análisis en R (Apéndice \hyperlink{apendice-alt}{A}), incluyendo instrucciones para hacer una curva de valores $p$ (*p-curve*; Apéndice \hyperlink{p-curve}{A1}), así como la manera correcta de citar paquetes de R (Apéndice \hyperlink{paquetes-cit}{B}) y, con fines de garantizar la reproducibilidad de este documento, una lista de los paquetes usados para la creación de esta guía, con sus respectivas versiones (Apéndice \hyperlink{paquetes-list}{C}).

---------------

# Base de datos de ejemplo

A lo largo de esta guía usaré la base de datos `dat.molloy2014`, tomada de @molloy2013. Esta base de datos viene incluida con el paquete `metafor` de R.

Básicamente, @molloy2013 estudiaron si existe una asociación entre la escrupulosidad (*conscientiousness*[^1]) y la adherencia a la medicación. En otras palabras, ¿las personas más *escrupulosas* tienden a cumplir más con la medicación prescrita?

[^1]: En el contexto de los rasgos de personalidad, la dimensión de la *escrupulosidad* (también llamada conciencia o "voluntad de logro"), se refiere al auto-control, incluyendo la planificación y ejecución de tareas. Para una definición detallada del concepto de escrupulosidad en este contexto, ver @johnBigFiveTrait1999.

Primero, debemos cargar los principales paquetes que usaré a lo largo de esta guía: `metafor` [@viechtbauer2010] y `metaviz` [@KossmeierMetaviz] para hacer e ilustrar los resultados del meta-análisis, así como `dplyr` [@WickhamDplyr2021] y `forecats` [@Wickhamforcats2022] para manipular y organizar la base de datos, y `ggplot2` [@Wickhamggplot22016] para hacer algunas gráficas más adelante[^2].

[^2]: Los paquetes `dplyr`, `forecats` y `ggplot2` hacen parte de [`tidyverse`](https://www.tidyverse.org/) [@Wickhamtidyverse2019], una colección de paquetes para ciencia de datos que comparten un mismo diseño y gramática. Por esto, en vez de cargar cada paquete independientemente, si instalas `tidyverse`, puedes cargar los paquetes de esta colección usando `library(tidyverse)`.

```{r eval = FALSE}
library(metafor)
library(metaviz)
library(dplyr)
library(forcats)
library(ggplot2)
```

Como ya hemos cargado el paquete `metafor`, ya podemos cargar la base de datos `dat.molloy2014`. En este caso, para poder *llamarla* cuando sea necesario, la asignaré a un objeto que llamaré simplemente `dat`.

```{r}
dat <- get(data(dat.molloy2014))
```

Tras asignar la base de datos a este objeto (`dat`), la base de datos se puede ver como *output*[^3] en la consola de R sencillamente usando como comando el nombre que le dimos al objeto al que la asignamos (en este caso, `dat`).

[^3]: En R y otros lenguajes de programación, se llama "*output*" a la *salida* o resultado visible de una función o proceso, en este caso en la consola (que en RStudio es normalmente el panel que está a la izquierda en la parte inferior).

```{r}
dat
```

## Variables de la base de datos {#variables-inf}

Por favor mira la base de datos, y familiarízate con sus columnas. ¿Qué información contiene cada variable? Explicaré cada una a continuación.

Primero, hay unas columnas que contienen información absolutamente necesaria para poder hacer un meta-análisis: 

-   `authors`: los autores de cada estudio a meta-analizar

-   `year`: año de publicación de cada estudio

-   `ni`: tamaño de muestra de cada estudio

-   `ri`: coeficiente de correlación de Pearson de cada estudio

Esas son las variables más importantes y son necesarias para hacer un meta-análisis de correlación simple: alguna columna o columnas con información que permita identificar cada estudio (en este caso `authors` y `year`), una columna con el tamaño de muestra de cada estudio (en este caso, `ni`), y finalmente una columna con los coeficientes de correlación (en este caso la columna `ri`). Si tu meta-análisis no requiere moderadores (ver sección \@ref(met-moderation)), con una estructura como esta sería suficiente.

Después hay unas columnas que contienen variables específicas para este este ejemplo, y que son una serie de posibles moderadores que agrupan características de los estudios a meta-analizar:

-   `controls`: cantidad de variables controladas ("`none`": ninguna  o "`multiple`": múltiples)

-   `design`: si se utilizó un diseño transversal ("`cross-section`") o prospectivo ("`prospective`") [para más información acerca de estos tipos de diseño, ver por ejemplo @Manterola2019]

-   `a_measure`: tipo de medida de adherencia ("`self-report`": autoinforme u "`other`": otro tipo de medida más *objetivo*)

-   `c_measure`: tipo de medida de escrupulosidad (si se midió con alguna versión del inventario de personalidad NEO o con alguna otra escala)

-   `meanage`: edad promedio de la muestra de cada estudio

-   `quality`: calidad metodológica (la calidad metodológica fue calificada por los autores del meta-análisis en una escala de 1 a 4, donde las puntuaciones más altas indican una mayor calidad; para información respecto a cómo se obtuvo esta puntuación, ver el artículo original de @molloy2013.

Por supuesto, el *output* de la consola no es la forma más clara de ver la base de datos, pero para una versión más legible, se puede usar la función `View`, y el nombre de la base de datos o tabla como argumento (en este caso `View(dat)`). Sin embargo, de aquí en adelante mostraré algunas tablas en un formato de impresión (y no como sale en la consola de R), para que sean más fáciles de leer.

Voy a volver a cargar la base de datos (sobrescribiendo el objeto `dat`), para organizarla un poco mejor. Primero, agregaré una nueva columna llamada `study_id`, en la que numeraré los estudios del 1 al 16, lo que será útil para identificar cada estudio en algunas gráficas y análisis. Después, reorganizaré las columnas para que `study_id` sea la primera, en vez de la última columna. Finalmente, las columnas de variables que son factores (`controls`, `design`, `a_measure`, `c_measure` y `quality`), pueden ser definidas como tal, para evitar pasos adicionales más adelante. En este caso, además, voy a reorganizar los niveles de la variable `controls` (primero `none` y después `multiple`); esto, aunque no es un paso necesario, puede ayudar con la interpretación de resultados.

```{r molloy2014}
dat <- get(data(dat.molloy2014)) |>
  mutate(study_id = 1:16)  |> # crear columna 'study_id' y agregar números del 1 al 16
  select(study_id, authors:quality) |> # mover 'study_id' como primera columna
  mutate_at(c("controls", # transformar variables relevantes en factores
              "design",
              "a_measure",
              "c_measure",
              "quality"), 
            as.factor) |> 
  mutate(controls = fct_relevel(controls, # reorganizar niveles de la variable 'controls'
                                "none", "multiple"))  
```

Con esto, la base de datos tiene ahora la siguiente estructura (Tabla \@ref(tab:estructuramod)):

```{r estructuramod, echo = FALSE, message = FALSE}
kable(dat, 
      linesep = "",
      booktabs = TRUE,
      caption = "Estructura de la base de datos con estudios numerados") |> 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) |> 
  footnote(general = "Datos tomados de Molloy et al. (2013).",
           general_title = "Nota:",
           footnote_as_chunk = TRUE)
```

## Preparación de los datos: transformación de coeficientes *r* de Pearson a *z* de Fisher

Los coeficientes de Pearson no se distribuyen normalmente, lo que podría llevar a calcular varianzas incorrectas, especialmente cuando se trata de correlaciones con tamaños de muestra pequeño [@shadishCombiningEstimatesEffect2009]. Por esto, lo mejor es transformar los coeficientes *r* de Pearson a *z* de Fisher [-@fisherFrequencyDistributionValues1915], que no tienen este problema[^4].

[^4]: Para decirlo de manera más precisa, el problema es que la distribución del coeficiente de correlación de Pearson (*r*), es muy sesgada cuando se trata de variables altamente correlacionadas (positiva, o negativamente). Esto dificulta la estimación de los intervalos de confianza y por tanto la aplicación de las pruebas de significación para coeficientes *r* [e.g., @shadishCombiningEstimatesEffect2009]. La transformación de Fisher de valores *r* a *z* ---que es la [tangente hiperbólica inversa](https://es.wikipedia.org/wiki/Tangente_hiperb%C3%B3lica) de *r*--- resuelve este problema, pues los coeficientes *z* tienen una distribución aproximadamente normal, y una varianza estable a lo largo de diferentes valores posibles de *r* [para una demostración en español, ver @sanchez-brunoTransformacionFisherPara2005].

Para transformar los coeficientes *r* de Pearson a coeficientes *z* de Fisher, usaré la función `escalc` del paquete `metafor`, que sirve para calcular diversos tamaños de efecto que se utilizan comúnmente en meta-análisis. Los argumentos que requiere esta función, además del tipo de transformación a realizar (en este caso `measure = "ZCOR"`), son los coeficientes de correlación (`ri`), el tamaño de muestra de cada correlación (`ni`), y la base de datos que contiene estos valores (`data`). En nuestro caso, las columnas donde están estos valores, tienen los mismos nombres (`ri`, `ni`). En este ejemplo, asignaré el resultado de esta función al mismo objeto `dat`, que contiene la base de datos, para sobrescribirlo y no crear objetos adicionales.

```{r}
dat <- escalc(measure = "ZCOR", ri = ri, ni = ni, data = dat)
```

Esta función agrega dos nuevas variables: `yi`, que es el tamaño de efecto (en valores *z* de Fisher), y `vi` que es la varianza (Tabla \@ref(tab:estructuramod2)).

```{r estructuramod2, echo = FALSE, message = FALSE}
kable(dat, 
      linesep = "",
      booktabs = TRUE,
      caption = "Estructura de la base de datos, con transformación de los r de Pearon a z de Fisher") |> 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) |> 
  footnote(general = "Las nuevas columnas creadas usando la función \\\\texttt{escalc} 
           (\\\\texttt{yi} como tamaño de efecto y \\\\texttt{vi} como varianza) están 
           resaltadas en naranja",
           general_title = "Nota:",
           footnote_as_chunk = TRUE,
           escape = FALSE) |> 
  column_spec(12:13, background = "#f68212")
```

---------------

# Cómo hacer el meta-análisis {#meta-cor}

Para hacer el meta-análisis, usaré la función `rma` del paquete `metafor`. Esta función requiere especificar los tamaños de efecto (`yi`) y varianzas (`vi`) de los estudios a meta-analizar. En nuestro caso, las columnas donde están estos valores, tienen los mismos nombres (`yi`, `vi`). Asignaré los resultados del meta-análisis a un nuevo objeto llamado `res`.

```{r}
res <- rma(yi = yi, vi = vi, data = dat)
```

Los resultados, son los siguientes:

```{r}
res
```

## Interpretación de los resultados del meta-análisis {#meta-interp}

Vamos a analizar estos resultados de la consola de R por partes:

Primero, nos confirman que ajustamos un modelo con efectos aleatorios (`Random-Effects Model`), a partir de 16 estudios (`k = 16`), y que para estimar $\tau^2$ (tau cuadrado) usamos el método de **máxima verosimilitud restringida**[^5] (`tau^2 estimator: REML`), que se designa como *REML* por sus siglas en inglés .

[^5]: Hay varios métodos disponibles como estimador, además de **máxima verosimilitud restringida** (REML). Sin embargo, si tienes dudas, REML es una buena opción. Cada método tiene ventajas y desventajas que, si tienes interés en mirar, están descritas en la [documentación](https://www.rdocumentation.org/packages/metafor/versions/2.4-0/topics/rma.uni) de la función `rma`.

Posteriormente, nos provee los valores de una serie de estimadores de heterogeneidad:

-   $\tau^2$: `tau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)`

-   $\tau$: `tau (square root of estimated tau^2 value): 0.0901`

-   $I^2$: `I^2 (total heterogeneity / total variability): 61.73%`, y

-   $H^2$: `H^2 (total variability / sampling variability):  2.61`

La tercera parte, reporta otra prueba de heterogeneidad, usando el estadístico $Q$:

-   `Test for Heterogeneity:`

    `Q(df = 15) = 38.1595, p-val = 0.0009`

De todos estos, los más comúnmente reportados son $\tau^2$, $\tau$, $I^2$ y $Q$. Cada una de estas medidas tiene ventajas y desventajas, por lo cual tiene sentido reportarlas todas.

-   $I^2$: tiene la ventaja de ser sencillo de interpretar, pues hay criterios generales para heterogeneidad baja, moderada y alta (típicamente 25%, 50%, and 75%, respectivamente). Sin embargo, es muy sensible a los tamaños de muestra de los estudios meta-analizados (por ejemplo, si en tu meta-análisis hay estudios con tamaños de muestra muy grandes, esto va a sesgar tu $I^2$).

-   $Q$: aunque no es sensible al tamaño de muestra, es sensible al número de estudios meta-analizados. Tiene la ventaja de ser un test de hipótesis, y como tal, puede ser interpretado a partir de su valor *p*.

-   $\tau^2$: no tiene problemas de sensibilidad a los tamaños de muestra o número de estudios meta-analizados, pero es más difícil de interpretar. $\tau^2$ es una estimación de la varianza de los tamaños de los efectos reales entre los estudios meta-analizados. Se usa, principalmente, para asignar pesos a cada estudio [para más información, ver @borensteinIdentifyingQuantifyingHeterogeneity2009].

En nuestro caso, el estadístico $Q$ sugiere que hay una heterogeneidad significativa en los estudios meta-analizados ($p$ = 0.0009). $I^2$, sugiere una heterogeneidad moderada, lo que quiere decir que se estima que más de la mitad (61.73%) de la varianza se deriva de diferencias en los tamaños de efecto.

Más abajo, el *output* de la consola de R nos muestra los resultados de nuestro meta-análisis (`Model results`); en otras palabras, ¿cuál es el tamaño de efecto de la asociación entre escrupulosidad (*conscientiousness*) y la adherencia a la medicación, según nuestro meta-análisis?

Esta parte nos provee varios resultados:

-   `Estimate (0.1499)`: estimado de la correlación entre escrupulosidad y adherencia a la medicación, en este caso en valores *z* de Fisher, pues así transformamos los coeficientes de cada estudio

-   `se (0.0316)`: error estándar del estimado de la asociación (en valores *z* de Fisher)

-   `zval (4.7501)`: estadístico *Z* (mayúscula) que comprueba la media de una distribución. No se debe confundir con la transformación de coeficientes de correlación a *z* de Fisher (minúscula); este estadístico no nos provee una estimación de la asociación entre las variables correlacionadas, sino, de manera similar a una prueba *t*, nos sugiere si nuestra media (para el caso, el resultado de nuestro meta-análisis), se diferencia de 0 (o una correlación nula). Cuando *Z* es mayor a 1.96 (o menor a -1.96), nuestro resultado está en el 5% extremo de la distribución *Z* y sería significativo con un $\alpha$ tradicional de 0.05 (dos colas)

-   `pval (<.0001)`: valor *p* de la correlación meta-analizada

-   `ci.lb (0.0881)`: límite inferior del intervalo de confianza (*confidence interval lower bound*) de la correlación meta-analizada (en valores *z* de Fisher)

-   `ci.ub (0.2118)`: límite superior del intervalo de confianza (*confidence interval upper bound*) de la correlación meta-analizada (en valores *z* de Fisher)

-   Nivel de significación (`***`): representación con asteriscos (o un punto) del nivel de significación

-   `Signif. codes: 0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1`: Clave para interpretar los niveles de significación. Aunque puede parecer complejo, básicamente, quiere decir que tres asteriscos (`***`) representan un valor *p* entre 0 y 0.001 (lo que comúnmente se representa como *p* \< .001); dos asteriscos (`**`) un valor *p* entre 0.001 y 0.01 (*p* \< .01); un asterisco (`*`) un valor *p* entre 0.01 y 0.05 (*p* \< .05); un punto (`.`) un valor *p* entre 0.05 y 0.01 (*p* \< .1, que ya no es significativo); y si no hay ningún símbolo, un valor *p* entre 0.1 y 1 (*p* > .1, no significativo)

En este caso, el meta-análisis nos sugiere que en efecto existe una asociación positiva entre escrupulosidad y adherencia a la medicación (coeficiente de correlación transformado en *z* de Fisher = .1499), con un error estándar de 0.0316. Así mismo, sugiere que esa asociación es significativa (*Z* = 4.7501, $p$ < .0001), y nos muestra el intervalo de confianza del 95% (o IC 95%); los intervalos de confianza (en este caso del 95%) lo que estiman es que, si hiciéramos 100 muestras independientes, 95 de estas contendrían una asociación que estaría entre los límites inferior (*z* = .0881) y superior (*z* = .2118) de estos intervalos de confianza.

### Reporte del meta-análisis básico {#reporte1}

Esto se podría resumir, por ejemplo, como:

```{=latex}
\begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
  colback=iacol!5!white,colframe=iacol!75!white,colbacktitle=iacol,
  title=Ejemplo de reporte básico,fonttitle=\bfseries,
  boxed title style={size=small,colframe=iacol} ]
  
Este meta-análisis examinó la relación entre la escrupulosidad y la adherencia a la medicación a través de 16 estudios. Los resultados indicaron una asociación significativa y positiva entre ambos factores ($z$ ± $se$ = 0.15 ± 0.032, IC 95\% [0.09, 0.21]; $Z$ = 4.75, $p$ < .0001). Sin embargo, se encontró una moderada pero significativa heterogeneidad en cuanto al tamaño del efecto entre los estudios incluidos ($\tau^2$ ± $se$ =  0.0081 ± 0.0055; $\tau$ = 0.0901; $Q$(15) =  38.16, $p$ < .001; $I^2$ = 61.7\%). Por tanto, es importante interpretar los resultados con precaución debido a la amplia variabilidad en el tamaño del efecto entre los estudios. A pesar de estas limitaciones, los hallazgos sugieren que la escrupulosidad puede tener un efecto positivo en la adherencia a la medicación.

\end{tcolorbox}
```

Siempre es buena idea citar tablas y figuras relevantes junto al reporte de los resultados que ilustran. Aunque aún no hemos creado ninguna figura, es buena idea al menos citar (e incluir) un *Forest plot* (diagrama de bosque; \@ref(forest-inf)), y un *Funnel plot* (diagrama de embudo; sección \@ref(funnel-inf)), elementos que explicaré más adelante.

### Alternativa: reportar el estimado como *r* de Pearson en vez de *z* de Fisher

Para reportar la correlación, si prefieres reportar coeficientes *r* de Pearson en vez de la transformación a *z* de Fisher, puedes transformar los valores *z* de Fisher de vuelta a *r* de Pearson. Para esto existen múltiples opciones en R, incluyendo simplemente usar la función `tanh`, que calcula la tangente hiperbólica, o la función `fisherz2r` del paquete `psych` [@revellePsych2021]. Por ejemplo, para transformar el estimado de nuestro meta-análisis a *r* de Pearson, sólo debo usar alguna de esas funciones, y agregar el valor *z* (0.1499 en nuestro caso) como único argumento:

```{r message = FALSE}
tanh(0.1499)
library(psych)
fisherz2r(0.1499)
```

Cualquiera de estas opciones nos da un valor de coeficiente de correlación de Pearson (*r* = 0.1487872), muy similar al obtenido como *z* de Fisher (*z* = 0.1499). Esto se debe a que, para coeficientes *r* de Pearson entre -0.4 y 0.4, la transformación a valores *z* de Fisher produce resultados muy similares (Fig. \@ref(fig:rvsz)). 

```{r rvsz, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 4, fig.cap = "Asociación entre coeficientes de correlación *r* de Pearson (eje *X*), y su transformación a *z* de Fisher (eje *Y*). La línea naranja representa la asociación entre valores *r* y *z*; como referencia, la línea negra punteada representa igualdad entre ejes (*y* = *x*). Como se puede ver, cuando *r* está aproximadamente entre -0.4 y 0.4 (rectángulo gris), los valores *r* y *z* son casi idénticos. Para valores más extremos, el valor de *z* se aleja progresivamente del valor de *r*."}

library(tidyquant)

rVsz <- tibble(r = c(seq(-0.999, -0.901, by = 0.001), 
                     seq(-0.9, 0.9, by = 0.01), 
                     seq(0.901, 0.999, by = 0.001))) |>
  mutate(z = atanh(r))
  
ggplot(rVsz, aes(x = r, y = z)) +
  annotate("rect", xmin = -0.4, xmax = 0.4, ymin = -Inf, ymax = Inf, alpha = 0.2) +
  annotate("text", x = 0, y = 0.2, 
                label = expression(paste("Mínima diferencia entre ", italic(r)," y ", italic(z))), 
           parse = TRUE,
           angle = 16,
           size = 2) +
  geom_smooth(se = FALSE, color = "#F68212") +
  geom_segment(aes(x = -1, xend = 1, y = -1, yend = 1), lty = "dotted", alpha = 0.1) +
  xlim(-1, 1) +
  scale_x_continuous(breaks = round(seq(-1, 1, by = 0.1),1)) +
  scale_y_continuous(breaks = round(seq(-3, 3, by = 0.2),1)) +
  labs(x =  expression(paste("Coeficiente de correlación (", italic("r"), ") de Pearson")),
       y = expression(paste("Transformación a ", italic(" z"), " de Fisher"))) +
  theme_tq()
```

Por supuesto, si decides reportar los resultados de tu meta-análisis en coeficientes *r* de Pearson, siempre puedes hacer lo mismo con el error estándar y los límites del intervalo de confianza del 95% (todos valores en *z* de Fisher, pues fue el tamaño de efecto que meta-analizamos).

## Más información sobre heterogeneidad {#heterog-inf}

Es importante tener en cuenta que la heterogeneidad no es un *supuesto* que se deba cumplir al hacer un meta-análisis, y por ende una heterogeneidad moderada o alta no invalida sus resultados. Sencillamente es información útil que se debe reportar y tener en cuenta al interpretar el resultado de un meta-análisis. En este caso, la presencia de heterogeneidad sugiere que los estudios meta-analizados varían y no suelen reportar resultados similares.

De manera general, y para decirlo de manera más técnica, la presencia de heterogeneidad estadística es indicativa de una variación entre los estudios en la magnitud y la dirección de la estimación del efecto estudiado [para más información y ejemplos, ver @sedgwickMetaanalysesWhatHeterogeneity2015]. 

Por esto, reportar información detallada acerca de la heterogeneidad de los estudios meta-analizados es siempre útil. De hecho, además de reportar los estadísticos $\tau^2$, $\tau$, $I^2$ y $Q$ (como expliqué en la sección \@ref(meta-interp)), podemos fácilmente calcular los intervalos de confianza para $\tau^2$, $\tau$, e $I^2$ (además de $H^2$, que no he usado) con la función `confint`.

```{r}
confint(res)
```

Para $\tau^2$, el hecho de que los límites del intervalo de confianza no crucen el 0 (es decir, no hay un límite negativo y otro positivo; en nuestro caso, ambos son positivos:  IC 95% [0.0017, 0.0378]), también sugiere que que hay heterogeneidad entre los estudios que meta-analizamos.

### Reporte del meta-análisis con intervalos de confianza para medidas de heterogeneidad {#reporte2}

Estos intervalos de confianza que también pueden ser reportados junto a sus correspondientes  estadísticos. Entonces, esto se podría resumir, por ejemplo, como:

```{=latex}
\begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
  colback=iacol!5!white,colframe=iacol!75!white,colbacktitle=iacol,
  title=Ejemplo de reporte con intervalos de confianza para heterogeneidad,fonttitle=\bfseries,
  boxed title style={size=small,colframe=iacol} ]
  
El meta-análisis incluyó 16 estudios que examinaron la relación entre la escrupulosidad y la adherencia a la medicación. Los estudios incluidos variaron en cuanto al diseño, la población de pacientes y la intervención evaluada. Los resultados del meta-análisis indicaron una asociación positiva significativa entre la escrupulosidad y la adherencia a la medicación ($z$ ± $se$ = 0.15 ± 0.032, IC 95\% [0.09, 0.21]; $Z$ = 4.75, $p$ < .0001). La heterogeneidad en torno al tamaño medio del efecto entre los estudios incluidos fue moderada pero significativa ($\tau^2$ ± $se$ =  0.0081 ± 0.0055, IC 95\% [0.0017, 0.0378]; $\tau$ = 0.0901, IC 95\% [0.0412, 0.1944]; $Q$(15) =  38.16, $p$ < .001; $I^2$ = 61.7\%, IC 95\% [25.28\%, 88.25\%]). Se observó una amplia variabilidad en el tamaño del efecto entre los estudios, lo que sugiere que los resultados deben interpretarse con precaución. A pesar de estas limitaciones, los hallazgos sugieren que es probable que la escrupulosidad tenga un efecto positivo en la adherencia a la medicación. Es importante tener en cuenta que la calidad metodológica de los estudios incluidos en el meta-análisis fue variable, lo que puede afectar la validez y generalización de las conclusiones.

\end{tcolorbox}
```

De nuevo, recuerda que siempre es buena idea citar tablas y figuras relevantes junto al reporte de los resultados que ilustran. Aunque aún no hemos creado ninguna figura, al reportar un meta-análisis es buena idea al menos citar (e incluir) un *Forest plot* (diagrama de bosque; \@ref(forest-inf)), y un *Funnel plot* (diagrama de embudo; sección \@ref(funnel-inf)). Más adelante explicaré cómo crear e interpretar estas figuras.

## Diagnóstico de influencia {#diag-inf}

Otro aspecto importante de un meta-análisis, es determinar si alguno o algunos de los estudios meta-analizados es o son particularmente influyentes en nuestro resultado[^6]. Para esto, se suele usar la técnica conocida como *leave-one-out*, que se refiere al resultado del meta-análisis cuando se excluye cada estudio; al estimar cómo y cuánto cambia el resultado del modelo de meta-análisis al excluir cada estudio, podemos estimar su influencia en el resultado. Dicho de otra manera, si al excluir un estudio el resultado cambia mucho, sabemos que ese estudio tiene gran influencia en el meta-análisis y sería mejor excluirlo. 

[^6]: Por ejemplo, si estuviésemos meta-analizando 20 estudios, de los cuales 19 tienen un *n* de 100, pero el otro tiene un *n* de 10.000, este último tendrá una influencia enorme en nuestro resultado. Sería preocupante que nuestro meta-análisis sea dependiente principalmente de un único estudio.

Para hacer un análisis de influencia, podemos usar la función `influence` del paquete `metafor`, cuyo resultado, en este caso, asignaré a un objeto llamado `inf.res`.

```{r}
inf.res <- influence(res)
```

Dado que lo asigné a un objeto (`inf.res`), para ver el resultado, debo usar como comando el nombre que le di al objeto.

```{r}
inf.res
```

Esto me muestra gran cantidad de datos de cada estudio (en este caso, lo presento como una tabla sin formato, tal cual se ve en la consola de R), y contiene información que puede parecer muy compleja; sin embargo, como verás, la última columna resume la interpretación general de estos resultados de influencia, facilitando muchísimo su interpretación. 

Las columnas que incluye este *output* son las siguientes:

-   `rstudent`: residuos estandarizados externamente. Esto **no** corresponde al coeficiente de correlación de cada estudio, sino a la diferencia entre el tamaño de efecto observado en cada estudio, y la predicción de dicho valor cuando dicho estudio se elimina del meta-análisis

-   `dffits`: diferencia de ajuste(s) (en inglés *difference in fit(s)*). Es una medida diagnóstica de la influencia de cada punto en una regresión (en este caso, cada estudio en un meta-análisis) propuesta originalmente por @belsleyRegressionDiagnosticsIdentifying1980

-   `cook.d`: Distancia de Cook (en inglés, *Cook's distance*). Es otra medida diagnóstica de la influencia, propuesta originalmente por @cookDetectionInfluentialObservation1977. Es conceptualmente idéntica a DFFITS (`dffits`), y de hecho existe una fórmula para convertir una medida en la otra [@Henry2003]. Aunque no hay un acuerdo absoluto al respecto, comúnmente se asume que valores mayores a 1 ($D_{i} > 1$), representan puntos (o, en este caso, estudios) influyentes en un modelo [@cookResidualsInfluenceRegression1982], que probablemente deban ser excluidos

-   `cov.r`: relación (o proporción) de covarianza. Es el determinante de la matriz de varianza-covarianza de las estimaciones de los parámetros basadas en el conjunto de datos, cuando cada estudio se elimina del meta-análisis, dividido por el mismo determinante de la matriz de varianza-covarianza cuando se incluyen todos los estudios. Sé que suena complejo, pero básicamente, un valor menor a 1 indica que la eliminación de ese estudio produce estimaciones más precisas de los coeficientes del modelo

-   `tau2.del`: es la heterogeneidad (residual) $\tau^2$ cuando se elimina cada estudio

-   `QE.del`: similar al resultado anterior, este se refiere al estadístico $Q$ obtenido cuando se excluye cada estudio

-   `hat`: los valores *hat* ($h$) son una medida estandarizada de la distancia de cada punto a la media de la variable predictora. Mientras valores cercanos a 0 indican que no hay un valor preocupante, valores cercanos a 1, aunque no indican directamente alta influencia de algún punto, ciertamente nos indican que vale la pena investigar más. Los valores *hat* están abiertos a la interpretación, pero un valor de corte que es común es el doble de la media de los todos valores *hat* ($\overline{h}$); cualquier estudio con un valor mayor debe ser examinado con cuidado

-   `weight`: peso asignado a cada estudio

-   `dfbs`: el valor de `dfbs` (o *DFBETAS*) indica cuántas desviaciones estándar cambia el coeficiente estimado después de excluir cada estudio del modelo de meta-análisis 

-   `inf`: por suerte, esta columna `inf` resume esta información por nosotros. Cualquier estudio que se considere influyente, teniendo en cuenta la diferencia de ajuste, la distancia de Cook, los valores *hat* o *DFBETAS*, será señalado acá como influyente, usando asteriscos

Aunque hay mucha información, lo más importante ahora es mirar la última columna, llamada `inf`. Si ahí aparecieran asteriscos para algún estudio meta-analizado (que no es nuestro caso), sugeriría que ese estudio es particularmente influyente y podría ser necesario eliminarlo del meta-análisis.

Por último, podemos también ver esta información que tenemos guardada en el objeto `inf.res`, de manera gráfica, usando la función `plot` (Fig. \@ref(fig:infplot)).

```{r infplot, fig.height = 6.6, fig.cap = "Diagnóstico de influencia. Estudios particularmente influyentes serían representados con un punto rojo. Los números 1 a 16 en el eje *X* representan cada estudio, como lo definimos en columna \\texttt{study\\_id} de la Tabla \\@ref(tab:estructuramod). En este caso, no hay ningún estudio que se considere demasiado influyente, por lo que este análisis sugiere que no es necesario excluir ningún estudio de nuestro meta-análisis."}
plot(inf.res)
```

Más adelante, en la sección \@ref(todas-combinaciones), daré más información y técnicas más completas para identificar estudios atípicos que podrían sesgar nuestra estimación del efecto *real*.

## *Forest plot* (diagrama de bosque) {#forest-inf}

Hacer un diagrama de bosque (*forest plot*) que resuma nuestro meta-análisis usando  [`metafor`](https://www.metafor-project.org/doku.php) es muy fácil; sólo tenemos que usar la función `forest`, usando como argumento el objeto al que asignamos los resultados de nuestro meta-análisis (en nuestro caso, `res`).

```{r eval = FALSE}
forest(res)
```

Esto produce la Figura \@ref(fig:for-plot1).

```{r for-plot1, echo = FALSE, fig.height = 4, fig.cap = "*Forest plot* básico de [metafor](https://www.metafor-project.org/doku.php). Para cada estudio meta-analizado, tenemos el efecto (correlación, en este caso en valores *z* de Fisher), así como sus intervalos de confianza entre corchetes. Esta misma información está representada gráficamente, con los cuadrados representando el efecto de cada estudio así como sus intervalos de confianza como barras de error, y el tamaño de muestra representado por el tamaño del cuadrado. Bajo estos resultados, tenemos nuestro meta-análisis, con el mismo formato en texto, pero representando el efecto y sus intervalos de confianza con un diamante."}
par(mar = c(4,0,0,0))
forest(res)
```

Como se puede ver en las Figuras \@ref(fig:for-plot1), \@ref(fig:for-plot2) y \@ref(fig:for-plot3b) (que son versiones del mismo *forest plot*), no es una sorpresa que el análisis nos sugiera bastante heterogeneidad; las correlaciones encontradas entre los diferentes estudios varían mucho (están entre `r min(dat$ri)` y `r max(dat$ri)`), y aunque son positivas en la mayoría de los casos, en algunos son prácticamente 0 o incluso negativas.

Para una versión más completa y anotada, también usando el *plot* básico de [`metafor`](https://www.metafor-project.org/doku.php), pero agregando encabezados de cada columna en español, nombres de los estudios meta-analizados[^7] así como una columna con los pesos dados a cada estudio, y detalles del modelo final[^8], podemos hacer algo como esto:

[^7]: En este caso, dado que tenemos los autores y los años de publicación de cada estudio en columnas separadas, pegué las columnas `authors` y `year` separadas por una coma y un espacio: `paste(dat$authors, dat$year, sep = ", ")` como argumento `slab`.

[^8]: Estas opciones están explicadas [acá](https://search.r-project.org/CRAN/refmans/metafor/html/forest.rma.html).

```{r eval = FALSE}
# forest plot con anotaciones adicionales
forest(res, 
       cex = 0.75, xlim = c(-1.6, 1.6),
       slab = paste(dat$authors, dat$year, sep = ", "), showweights = TRUE,
       xlab = "Coeficiente de correlación (z de Fisher)", digits = c(2,3L),
       mlab = bquote(paste("Modelo EA: Q(", .(res$k - res$p), ") = ",
                           .(formatC(res$QE, digits = 2, format = "f")), ", p ", 
                           .(scales::pvalue(res$pval)), "; ", I^2, " = ",
                           .(formatC(res$I2, digits = 1, format = "f")), "%")))
# agregar encabezados a las columnas (valores de X y Y deben ser ajustados)
op <- par(cex = 0.8, font = 2) 
text(x = -1.6, y = 18, labels = "Autor(es), Año", pos = 4)
text(x = 0, y = 18, labels = "Efecto e IC", pos = 4)
text(x = 1, y = 18, labels = "Peso", pos = 2)
text(x = 1.6, y = 18, labels = "Corr. [95% IC]", pos = 2)
```

```{r for-plot2, echo = FALSE, fig.height = 4, fig.cap = "*Forest plot* anotado, creado con [metafor](https://www.metafor-project.org/doku.php). En esta versión agregué algunos encabezados en español, así como estadísticos generales del modelo de meta-análisis. Modelo EA se refiere al modelo meta-analizado, de efectos aleatorios."}
# forest plot con anotaciones adicionales
par(mar = c(4,0,0,0))
forest(res, cex = 0.75, xlim = c(-1.6, 1.6),
       slab = paste(dat$authors, dat$year, sep = ", "),
       showweights = TRUE,
       xlab = "Coeficiente de correlación (z de Fisher)",
       digits = c(2,3L),
       mlab = bquote(paste("Modelo EA: Q(", .(res$k - res$p), ") = ",
                           .(formatC(res$QE, digits = 2, format = "f")), ", p ", 
                           .(scales::pvalue(res$pval)), "; ", I^2, " = ",
                           .(formatC(res$I2, digits = 1, format = "f")), "%")))
# agregar encabezados a las columnas (valores de X y Y deben ser ajustados)
op <- par(cex = 0.8, font = 2) 
text(x = -1.6, y = 18, labels = "Autor(es), Año", pos = 4)
text(x = 0, y = 18, labels = "Efecto e IC", pos = 4)
text(x = 1, y = 18, labels = "Peso", pos = 2)
text(x = 1.6, y = 18, labels = "Corr. [95% IC]", pos = 2)
```

O, para un *forest plot* quizás más sofisticado, se puede usar la función [`viz_forest`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html#creating-forest-plots-with-function-viz_forest) del paquete [`metaviz`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html), en alguna de sus variantes y colores.

```{r eval = FALSE}
# A. Variante "classic" (no tiene que ser definida, pues es la opción por defecto)
viz_forest(res, 
           study_labels = paste(dat$authors, dat$year, sep = ", "),
           xlab = "Correlación", 
           annotate_CI = TRUE,
           summary_label = "Resumen",
           text_size = 2.6,
           x_trans_function = tanh)

# B. Variante "thick"
viz_forest(res, 
           study_labels = paste(dat$authors, dat$year, sep = ", "),
           xlab = "Correlación", 
           variant = "thick",
           col = "Greens",
           annotate_CI = TRUE,
           summary_label = "Resumen",
           text_size = 2.6,
           x_trans_function = tanh)

# C. Variante "rain"
viz_forest(res, 
           study_labels = paste(dat$authors, dat$year, sep = ", "),
           xlab = "Correlación", 
           variant = "rain",
           col = "Oranges",
           annotate_CI = TRUE,
           summary_label = "Resumen",
           text_size = 2.6,
           x_trans_function = tanh)
```

Con el código anterior genero las siguientes tres versiones del mismo *forest plot* (Fig. \@ref(fig:for-plot3b)) usando diferentes variantes y escalas de colores, y transformando de vuelta los coeficientes de *z* de Fisher a *r* de Pearson (con el argumento `x_trans_function = tanh`). Por supuesto, es cuestión de gusto cuál usar.

```{r for-plot3, echo = FALSE, fig.height = 7}
library(metaviz)
p.classic <- viz_forest(res, 
                        study_labels = paste(dat$authors, dat$year, sep = ", "),
                        xlab = "Correlación", 
                        annotate_CI = TRUE,
                        summary_label = "Resumen",
                        text_size = 2.6,
                        x_trans_function = tanh)

p.thick <- viz_forest(res, 
                      study_labels = paste(dat$authors, dat$year, sep = ", "),
                      xlab = "Correlación", 
                      variant = "thick",
                      col = "Greens",
                      annotate_CI = TRUE,
                      summary_label = "Resumen",
                      text_size = 2.6,
                      x_trans_function = tanh)

ggarrange(p.classic, p.thick,
          labels = "AUTO",
          nrow = 2)
```

```{r for-plot3b, echo = FALSE, fig.height = 3.5, warning = FALSE, fig.cap = 'Variantes de *forest plots* creados con  [metaviz](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html). **A.** Variante clásica (opción por defecto). **B.** Variante "thick" y escala de colores "Greens". **C.** Variante "rain" y escala de colores "Oranges".'}
p.rain <- viz_forest(res,
                     study_labels = paste(dat$authors, dat$year, sep = ", "),
                     xlab = "Correlación", 
                     variant = "rain",
                     col = "Oranges",
                     annotate_CI = TRUE,
                     summary_label = "Resumen",
                     text_size = 2.6,
                     x_trans_function = tanh)

ggarrange(p.rain,
          labels = c("C"))
```

## *Funnel plot* (diagrama de embudo), regresión de Egger y sesgo de estudios pequeños {#funnel-inf}

La creación e interpretación del *Funnel plot* es crucial, y aporta información muy importante acerca de un meta-análisis. Sin embargo, es en donde más errores se cometen. 

### Errores en la interpretación del *Funnel plot* y la regresión de Egger {#err-funnel-egger}

El principal error que la mayoría de los investigadores podemos cometer[^9], es concluir que un meta-análisis tiene (o no) riesgo de sufrir de sesgos de publicación, simplemente basándonos en la evaluación de la asimetría en el *funnel plot* y la regresión de Egger [@eggerBiasMetaanalysisDetected1997] (sección \@ref(reg-egger)).

[^9]: Me incluyo pues por supuesto también soy culpable de haber cometido este error en el pasado, incluyendo algunas ideas que expuse en mi video sobre meta-análisis en *jamovi* [@leongomezMetaanalysis2021].

Estos métodos, sin embargo, no son pruebas exclusivas de sesgo de publicación, sino de sesgo de estudios de tamaño muestral pequeño [ver e.g. @schwarzerSmallStudyEffectsMetaAnalysis2015], que pueden incluir sesgo de publicación, pero no se centran exclusivamente en esto (explico algunas técnicas específicas para estimar el sesgo de publicación en la sección \@ref(sesgo-pub)).

A pesar de esto, tanto la regresión de Egger como el *funnel plot* son interesantes, pues es importante considerar el sesgo que podrían generar estudios con tamaño de muestra pequeño en nuestra estimación del efecto *real* (o, para decirlo más técnicamente, el sesgo que podrían introducir estudios con poder estadístico bajo).

### *Funnel plot*

Para crear un *funnel plot* de nuestro meta-análisis con [`metafor`](https://www.metafor-project.org/doku.php) sólo tenemos que usar la función `funnel`, usando como argumento el objeto al que asignamos los resultados de nuestro meta-análisis (`res`).

```{r eval = FALSE}
funnel(res)
```

Con esto, generamos la Figura \@ref(fig:funnel-plot1).

```{r funnel-plot1, echo = FALSE, fig.height = 3.4, fig.cap = "*Funnel plot* básico de [metafor](https://www.metafor-project.org/doku.php). Para cada estudio meta-analizado, tenemos el efecto (correlación, en este caso en valores *z* de Fisher) en el eje *X*, así como su error estándar en el eje *Y*. La línea punteada vertical representa el efecto *real* estimado mediante nuestro meta-análisis (*z* = 0.15), así que podemos ver los estudios que encontraron un efecto mayor (derecha) o menor (izquierda) que éste. A primera vista no parece haber mucha asimetría, pero es importante tener en cuenta que es un análisis muy subjetivo."}
par(mar = c(4,4,0,1))
funnel(res)
```

O, si queremos poner los títulos de los ejes en español, podemos hacerlo agregando los argumentos `xlab` (para el eje $X$) y/o `ylab` (para el eje $Y$), como se ve en la Figura \@ref(fig:funnel-plot1a).

```{r eval = FALSE}
funnel(res, 
       xlab = "Coeficiente de correlación (z de Fisher)",
       ylab = "Error estándar")
```

```{r funnel-plot1a, echo = FALSE, fig.height = 3.4, fig.cap = "*Funnel plot* básico de [metafor](https://www.metafor-project.org/doku.php), idéntico al de la Figura \\@ref(fig:funnel-plot1), pero con títulos de ejes en español."}
par(mar = c(4,4,0,1))
funnel(res, 
       xlab = "Coeficiente de correlación (z de Fisher)",
       ylab = "Error estándar")
```

De nuevo, para una versión más sofisticada, se puede usar el paquete [`metaviz`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html), usando la función [`viz_funnel`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html#creating-funnel-plots-with-viz_funnel). Hay muchas opciones, pero como ejemplo, usaré la versión por defecto, agregando sólo la línea de la regresión de Egger (`egger = TRUE`; ver sección \@ref(reg-egger), más adelante), transformando los tamaños de efecto de regreso a $r$ de Pearson (`x_trans_function = tanh`), y con los títulos de los ejes en español (Fig. \@ref(fig:funnel-plot2)).

```{r funnel-plot2, fig.height = 3.5, warning = FALSE, fig.cap = "*Funnel plot* creado con [metaviz](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html). En azul, se representa el área donde estudios, según su error (y su tamaño de muestra), tendrían un efecto significativo al 5% (i.e. $p$ < 0.05), y fuera de esta, donde tendrían un efecto significativo al 1% (i.e. $p$ < 0.01). La línea negra vertical representa el efecto meta-analizado, y el triángulo a partir de su inicio, el área donde se ubican los estudios que no se diferencian significativamente del resultado del meta-análisis. La línea roja punteada, representa la regresión de Egger."}
viz_funnel(res, 
           egger = TRUE,
           x_trans_function = tanh,
           ylab = "Error estándar",
           xlab = "Coeficiente de correlación")
```

Alternativamente, el paquete [`metaviz`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html) tiene la función [`viz_sunset`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html#sunset-power-enhanced-funnel-plots), que permite además mostrar el poder estadístico (o potencia) de los estudios meta-analizados para detectar un efecto de interés mediante una prueba de Wald de dos colas. De ser necesario, para entender las bases del poder estadístico, recomiendo ver [esta serie de videos](https://youtube.com/playlist?list = PLHk7UNt35ccVdyHqnQ6oXVYA6JBNFrE1x) [@leongomezPoderRvid2020] y/o, para mayor profundidad, leer [esta guía](https://doi.org/10.5281/zenodo.3988776) [@leongomezAnalisisPoderEstadistico2020] que publiqué anteriormente.

A continuación, muestro dos versiones de *funnel plots* creados con la función `viz_sunset` (Fig. \@ref(fig:funnel-plot3)). En ambos casos, agregué el efecto *real* estimado a partir del meta-análisis (`contours = TRUE`), y transformé los tamaños de efecto de regreso a $r$ de Pearson (`x_trans_function = tanh`).

```{r eval = FALSE}
# A. Escala de poder discreta
viz_sunset(res,
           contours = TRUE,
           x_trans_function = tanh)

# B. Escala de poder contínua
viz_sunset(res, 
           contours = TRUE,
           x_trans_function = tanh, 
           power_contours = "continuous")
```

```{r funnel-plot3, message = TRUE, echo = FALSE, fig.height = 8.5, fig.width = 6, warning = FALSE, fig.cap = "Dos versiones de *funnel plot* creados con [metaviz](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html), usando la función viz-sunset, que estima el poder de cada estudio para detectar un efecto de interés. **A.** Poder representado por bandas dicretas de color. **B.** Poder representado de manera contínua en una escala de color. En ambos casos, y tal como en la Figura \\@ref(fig:funnel-plot2), el efecto real está representado como una línea vertical, y el triángulo a partir de su inicio representa el área donde se ubican los estudios que no se diferencian significativamente del resultado del meta-análisis."}
p.suna <- viz_sunset(res, 
                     contours = TRUE,
                     x_trans_function = tanh)
p.sunb <- viz_sunset(res, 
                     contours = TRUE,
                     x_trans_function = tanh,
                     power_contours = "continuous")
  
ggarrange(p.suna, p.sunb,
          labels = "AUTO",
          nrow = 2)
```

### Regresión de Egger {#reg-egger}

Para hacer una prueba formal de sesgo de estudios pequeños [@sternePublicationRelatedBias2000;  @sterneRecommendationsExaminingInterpreting2011; @schwarzerSmallStudyEffectsMetaAnalysis2015], a partir de la asimetría en el *funnel plot*, podemos hacer una prueba o regresión de Egger [@eggerBiasMetaanalysisDetected1997]. 

Es importante tener en cuenta que la regresión de Egger puede carecer de poder para detectar sesgos cuando el número de estudios es pequeño; @sterneRecommendationsExaminingInterpreting2011, de hecho, recomiendan no usar este técnica cuando tenemos menos de 10 estudios ($k < 10$). En nuestro ejemplo, dado que tenemos 16 estudios ($k = 16$), esto no debería ser un problema. 

En [`metafor`](https://www.metafor-project.org/doku.php), podemos hacer una regresión de Egger con la función `regtest`, de nuevo usando como único argumento el objeto al que asignamos el resultado de nuestro meta-análisis (`res`).

```{r eval = FALSE}
regtest(res)
```

Como se puede ver, en este caso la prueba de Egger no muestra un resultado significativo (`Test for Funnel Plot Asymmetry: z = 1.0216, p = 0.3070`), lo que sugiere que no hay sesgo de estudios pequeños.

```{r echo = FALSE}
regtest(res)
```

Con base en esto, y la inspección visual subjetiva del *funnel plot*, muchos investigadores podrían concluir, sin sustento adecuado, que en el meta-análisis no hay sesgo de publicación. Sin embargo, como mencioné antes (sección \@ref(err-funnel-egger)), estas pruebas no se centran en el sesgo de publicación sino en el sesgo de estudios pequeños. En otras palabras, con base en esto, lo único que podemos concluir correctamente, es que no hay sesgo de estudios pequeños (en la sección \@ref(sesgo-pub) a continuación, explicaré cómo evaluar si hay sesgo de publicación).

## Sesgo de publicación (*publication bias*) {#sesgo-pub}

Existen diferentes opciones para estimar el sesgo de publicación de un meta-análisis. En esta sección, me centraré en dos: el método de recorte y relleno (*trim and fill*; sección \@ref(trim-fill)), y el modelo de función de peso de Vevea y Hedges (sección \@ref(vevea-hedges)), que es uno de los modelos de selección más conocidos para evaluar sesgos de publicación.

### Método *trim and fill* (recorte y relleno) {#trim-fill}

El método de recorte y relleno (*trim and fill*) es una técnica no paramétrica para aumentar datos [@duvalNonparametricTrimFill2000; @duvalTrimFillSimple2000; ver también @duvalTrimFillMethod2005]. Este método estima los estudios potencialmente ausentes debido al sesgo de publicación en el gráfico de embudo, y ajusta la estimación del efecto *real*. Aunque muy útil, es importante tener en cuenta que este método tiene algunas limitaciones:

-  No debe entenderse como una forma de obtener una estimación más "válida" del efecto *real*, sino como una forma de examinar qué tan sensibles son los resultados a una forma particular de sesgo de publicación

-  No se puede usar en modelos con moderadores, como los que explico en la sección \@ref(met-moderation)

En [`metafor`](https://www.metafor-project.org/doku.php), el método de recorte y relleno (*trim and fill*) se hace simplemente con la función `trimfill`, de nuevo usando como argumento el objeto al que asignamos el resultado de nuestro meta-análisis (`res`). En este caso, asignaré el resultado de esta función a un objeto que llamaré `tf`.

```{r}
tf <- trimfill(res)
tf
```

Esto nos muestra unos resultados equivalentes y muy similares a los de nuestro meta-análisis original (sección \@ref(meta-cor)) y que se deben interpretar de manera igual. Sin embargo, hay unos pequeños cambios. 

Primero, comienza diciendo que el número estimado de estudios que faltan en el lado izquierdo es 2 (`Estimated number of missing studies on the left side: 2 (SE = 2.7118)`), lo que quiere decir que estimó que faltaban dos estudios al lado izquierdo de nuestro *funnel plot*, y no se recortó ninguno. Por supuesto, al sumar estos dos estudios hipotéticos, el meta-análisis se hace con base en *k* = 18 estudios, en vez de 16 (`Random-Effects Model (k = 18; tau^2 estimator: REML)`). Como los dos estudios hipotéticos que se agregaron están a la izquierda de nuestro *funnel plot*, el efecto que estimamos usando el método *trim and fill* es menor (*z* = 0.13, como se ve en los resultados de nuestro meta-análisis (`Model results`: `Estimate (0.1288)`).

Podemos hacer un *funnel plot* del meta-análisis con el método de recorte y relleno (*trim and fill*), usando como argumento el objeto al que asignamos los resultados (`tf`). Con esto, generamos la Figura \@ref(fig:tf-plot1).

```{r tf-plot1, fig.cap = "*Funnel plot* básico de [metafor](https://www.metafor-project.org/doku.php) usando el método de recorte y relleno (*trim and fill*). En negro los estudios meta-analizados; en blanco, los estudios *rellenados*."}
funnel(tf, 
       xlab = "Coeficiente de correlación (z de Fisher)",
       ylab = "Error estándar")
```

De nuevo, alternativamente podemos usar la función `viz_funnel` del paquete [`metaviz`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html), para crear este *funnel plot* representando los estudios *rellenados* con el método de recorte y relleno. Sin embargo, para esto usaremos como argumento el meta-análisis original (`res`), pero agregando los argumentos `trim_and_fill = TRUE` y `trim_and_fill_side = "left"` ("left" dado que sabemos que los estudios *faltantes* están a la izquierd). 

```{r tf-plot2, cache = TRUE, fig.height = 3.5, fig.cap = "*Funnel plot* creado con [metaviz](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html) usando el método de recorte y relleno (*trim and fill*). En blanco los estudios meta-analizados; en negro, los estudios *rellenados*."}
viz_funnel(res, 
           contours_col = "Oranges",
           trim_and_fill = TRUE, 
           trim_and_fill_side = "left", #IMPORTANTE
           egger = TRUE,
           x_trans_function = tanh,
           ylab = "Error estándar",
           xlab = "Coeficiente de correlación") +
  geom_vline(xintercept = 0, linetype = "dotted")
```

El resultado nos muestra que el análisis aumentó dos estudios hipotéticos a la izquierda. Es decir, sugiere que hay *cierto* nivel de sesgo de publicación. De hecho, la estimación del efecto al aumentar estos estudios hipotéticos para que la distribución sea realmente simétrica, disminuye a 0.1288, algo menor que el 0.1499 que nos sugería el análisis inicial. Se debe tener en cuenta que los estudios generados por el método *trim and fill*, son hipotéticos. Y que estos estudios se utilizan para estimar el tamaño y la dirección del sesgo de publicación, pero no son datos reales.Por esto, los resultados deben ser interpretados con precaución y no constituyen evidencia definitiva de sesgo de publicación.

Si quieres profundizar en el método de recorte y relleno (*trim and fill*), su interpretación, así como sus fortalezas y limitaciones, te recomiendo leer el artículo de @shiTrimandfill2019 al respecto.

### Modelo de función de peso de Vevea y Hedges {#vevea-hedges}

El modelo de función de peso de Vevea y Hedges [-@veveaGeneralLinearModel1995; ver también @veveaPublicationBiasResearch2005] es uno de los modelos de selección (en inglés, *selection models*) más conocidos[^10].

[^10]: El otro modelo de selección muy conocido, es el modelo de Copas y Shi [-@copasMetaanalysisFunnelPlots2000; -@copasSensitivityAnalysisPublication2001]. Aunque no lo incluyo en esta guía, es importante mencionar que hay evidencia que muestra que es preferible a métodos como el de *trim and fill* [@schwarzerEmpiricalEvaluationSuggests2010]. En R, este método se puede implementar con la función `copas` del paquete `metasens` [@schwarzerMetasensPackage; ver también Capítulo 5 en @schwarzerMetaAnalysis2015].

Los modelos de selección intentan describir el mecanismo de supresión del efecto y combinarlo con un modelo de tamaño del efecto que describa la distribución de los efectos en ausencia de sesgo de publicación.

El modelo de Vevea y Hedges da más peso a ciertos tamaños de efecto. La realidad de la literatura científica es que es más probable que algunos estudios sean publicados, dependiendo de sus valores $p$ [@coburnPublicationBiasFunction2015; para una descripción sencilla y general, ver por ejemplo @SesgoPublicacion2008]. En particular, es más probable que sean publicados los estudios con resultados *significativos* (típicamente, $p < 0.05$, usando un $\alpha$ tradicional de 0.05), en comparación con estudios con resultados no significativos (es decir, con resultados que no permiten rechazar la hipótesis nula). Estos sesgos pueden por supuesto sesgar la literatura publicada, y afectar nuestro meta-análisis.

En R, para estimar el sesgo de publicación de un meta-análisis usando el modelo de función de peso de Vevea y Hedges, se puede usar la función `weightfunct` del paquete `weightr` [@coburnWeightr2019].

Tal como se describe en la [documentación](https://www.rdocumentation.org/packages/weightr/versions/2.0.2/topics/weightfunct) de la función `weightfunct`, esta función nos permite "estimar tanto el modelo de función de peso para el sesgo de publicación que se publicó originalmente en @veveaGeneralLinearModel1995 como la versión modificada presentada en @veveaPublicationBiasResearch2005".

En este caso, usaré esta función, asignando el resultado a un objeto que llamaré `wf`.

```{r cache = TRUE}
library(weightr)
wf <- weightfunct(effect = dat$yi, v = dat$vi, table = TRUE)
wf
```

Los resultados tienen una estructura similar a los del meta-análisis original (descrito y explicado en la sección \@ref(meta-cor)) y a los del meta-análisis usando el método *trim and fill* (sección \@ref(trim-fill)). Sin embargo, se organizan en dos partes; primero presenta los resultados del modelo no ajustado (bajo el encabezado `Unadjusted Model (k = 16)`), y después presenta los resultados del modelo con pesos ajustados de acuerdo al modelo de función de peso de Vevea y Hedges (bajo el encabezado `Adjusted Model (k = 16)`).

El modelo no ajustado nos da un estimado muy similar (pero no idéntico) al del estudio original (0.1486), dado que usa un método ligeramente diferente.

Como antes, también nos da valores de heterogeneidad $\tau^2$, $\tau$ y $Q$.

Pero lo más importante, es que en la segunda parte (el modelo ajustado) los resultados del meta-análisis se calculan de acuerdo con los pesos dados a cada efecto. 

Esta técnica, a diferencia del método *trim and fill* no "recorta" ni "rellena" estudios, por lo que el análisis se hace con el mismo número de estudios (en este caso, 16; por eso el encabezado dice `Adjusted Model (k = 16)`).

Sin embargo, la función `weightfunct` incrementa el peso de estudios que tienen menos probabilidad de ser publicados (típicamente resultados no significativos), y reduce el peso de estudios con mayor probabilidad de ser publicados (los que encuentran resultados más extremos). Por esto, al usar esta técnica, estás asumiendo que, de hecho, en el efecto que tratas de encontrar en tu meta-análisis hay un sesgo de publicación, lo que a menudo es una suposición bastante apropiada.

Al usar esta técnica, tenemos un resultado bastante distinto. Mientras que el meta-análisis original nos daba como resultado un efecto de \~0.15, esta técnica nos estima un efecto de \~0.09. Ha *encogido* nuestro tamaño de efecto, asumiendo que nuestro resultado original estaba sesgado.

Al final, está el *Likelihood ratio test* (en español, "Prueba de la razón de verosimilitud"), que evalúa la bondad del ajuste de dos modelos estadísticos que compiten entre sí, con base en la relación de su verosimilitud[^11]. En este caso, comparando el modelo original, con este modelo con pesos ajustados.

[^11]: Aunque no voy a entrar en detalles respecto a técnicas de inferencia estadística basadas en verosimilitud, si quieres conocer más acerca de estos métodos, así como de estadística Bayesiana (que tiene mucho que ver con verosimilitud) y frecuentista (la que solemos usar, que hace inferencia a partir de valores $p$), con énfasis en las fortalezas y limitaciones de cada enfoque, recomiendo encarecidamente el libro *"Improving Your Statistical Inferences"* de Daniël Lakens [-@lakensImprovingYourStatistical2022]. Este libro, completamente gratuito y abierto, integra además los contenidos de dos cursos gratuitos del mismo autor (*"[Improving Your Statistical Inferences](https://www.coursera.org/learn/statistical-inferences)"* y *"[Improving Your Statistical Questions](https://www.coursera.org/learn/improving-statistical-questions)"*), que también recomiendo hacer a cualquier persona que quiera dedicarse a hacer investigación y use técnicas estadísticas.

Este resultado nos da una tendencia no descartable (`p-val = 0.084043`; menor que 0.10 y por tanto significativa si asumimos un análisis de una cola), que nos da evidencia de que en efecto hay un sesgo de publicación, a pesar de que el *funnel plot* (Figs. \@ref(fig:funnel-plot1), \@ref(fig:funnel-plot1a), \@ref(fig:funnel-plot2) y \@ref(fig:funnel-plot3)) y la regresión de Egger (sección \@ref(reg-egger)), sugerían lo contrario.

## Poder estadístico del meta-análisis {#poder-inf}

En esta sección explicaré cómo hacer un análisis de poder de un meta-análisis. 
Como mencioné anteriormente, de ser necesario, para entender las bases del poder estadístico, recomiendo ver [esta serie de videos](https://youtube.com/playlist?list = PLHk7UNt35ccVdyHqnQ6oXVYA6JBNFrE1x) [@leongomezPoderRvid2020] y/o, para mayor profundidad, leer [esta guía](https://doi.org/10.5281/zenodo.3988776) [@leongomezAnalisisPoderEstadistico2020] que publiqué anteriormente.

La idea de esto es saber si nuestro meta-análisis tiene un poder suficiente para detectar el efecto *real* estimado mediante nuestro análisis (en nuestro caso *z* = `r round(min(res$beta[1]), 2)` para el meta-análisis original "`res`", o *z* = `r round(wf$output_adj$par[2], 2)` el meta-análisis con pesos ajustados "`wf`"). 

Para este ejemplo, asumiré que el efecto *real* es el estimado en nuestro análisis original (*z* = `r round(min(res$beta[1]), 2)`), pues este efecto es mayor. Si no tuviésemos el poder suficiente para detectar confiablemente ese efecto, menos lo tendríamos para un efecto menor, como el detectado en nuestro meta-análisis con pesos ajustados.

Para hacer esto, usaré el paquete `metameta` [@quintanaGuideMetaPower;@quintanaMetameta2022], que permite calcular y visualizar el poder estadístico de un meta-análisis para detectar un rango de posibles efectos *reales* estimados.

### Instalación de `metameta`

El paquete `metameta` se debe instalar desde GitHub ya que, al día de hoy, no está aún disponible en CRAN[^12].

[^12]: [CRAN](https://cran.r-project.org/) (siglas de *Comprehensive R Archive Network*) es una red de servidores de todo el mundo que almacenan versiones idénticas y actualizadas del código y la documentación de R (incluyendo el lenguaje mismo de R, paquetes aceptados por el equipo de CRAN, y toda la documentación). Es el repositorio oficial y dedicado a R, desde el que normalmente instalas cualquier paquete. [GitHub](https://github.com/), en cambio, es un repositorio general y abierto para proyectos (típicamente programación) en el que, entre otras cosas, suelen estar alojados todos los paquetes de R en versiones de desarrollador, y antes de ser aceptados en CRAN. Por supuesto, a diferencia de CRAN, GitHub es un repositorio de código general, no específico para R.

Para esto, debemos tener instalado el paquete `devtools`, y usar la función `install_github` que nos permite instalar paquetes directamente desde GitHub.

```{r eval = FALSE}
#se debe tener instalado el paquete devtools
library(devtools)
install_github("dsquintana/metameta")
```

### Análisis de poder

Una vez hemos instalado `metameta`, podemos cargar el paquete.

```{r}
library(metameta)
```

Como datos, necesitamos no solamente los tamaños de efecto a meta-analizar ($r$ de Pearson transformado a *z* de Fisher), sino además los intervalos de confianza, tal como fueron reportados en varios de nuestros *Forest plots*.

Como mencioné, voy a asumir un efecto *real* de $r = 0.15$, tal como en nuestro meta-análisis original. Sin embargo, el efecto *real* no es algo que podamos saber con absoluta certeza (es, de hecho, lo que queremos acercarnos a conocer a través del meta-análisis), así que la función `mapower_ul` del paquete `metameta` calcula el poder de cada meta-análisis para un rango de posibles efectos reales.

Para esto, la función `mapower_ul` requiere una base de datos que contenga información concreta, y con columnas con nombres específicos: el efecto encontrado en cada estudio (en nuestro caso `yi`, que es la columna en la que guardamos los coeficientes de correlación transformados a *z* de Fisher), así como sus intervalos de confianza. Estas columnas deben llamarse `yi`, `lower` y `upper`, respectivamente.

Esto lo puedo obtener de manera sencilla con la función `summary`, que agrega a la base de datos (`dat`) los límites inferior (`ci.lb`) y superior (`ci.ub`) de los intervalos de confianza. Sin embargo, aunque la columna con el tamaño de efecto (`yi`) ya tiene el nombre requerido, debo renombrar las columnas que tienen los límites de los intervalos de confianza (`ci.lb` y `ci.ub`) como `lower` y `upper`, usando la función `rename`. 

```{r}
dat.poder <- summary(dat) |>
  rename(lower = ci.lb, upper = ci.ub)
```

Asigné esta versión modificada de la base de datos a un objeto que llamé `dat.poder`. Ya con esta base de datos lista y con el formato correcto para usar la función `mapower_ul`, podemos seguir adelante. 

La función `mapower_ul` requiere que asignemos la base de datos (`dat.poder`) al argumento `dat`, el tamaño de efecto que estimamos como efecto *real* (en este caso, 0.15) al argumento `observed_es`, así como un nombre que le daremos al meta-análisis al argumento `name` (en este caso lo llamaré "Molloy et al. 2013", pero por supuesto tú puedes darle el nombre que consideres adecuado, siempre entre comillas). Asignaré el resultado de este análisis a un objeto que simplemente llamaré `poder`.

```{r}
poder <- mapower_ul(dat = dat.poder, observed_es = 0.15, name = "Molloy et al. 2013")
```

Como veremos a continuación, este objeto tiene varios elementos muy importantes.

#### Poder mediano del meta-análisis para diferentes tamaños de efecto. {#poder-mediano}

El objeto generado por la función `mapower_ul` tiene varios elementos. Uno particularmente relevante es la lista de poder mediano estimado para diferentes tamaños de efecto (`$power_median_dat`), que asignaré como lista a un objeto que llamaré `lista_poder` (guardar este objeto es sumamente útil para hacer visualizaciones del análisis de poder, como explico más adelante, en la sección \@ref(firepower-plot)).

```{r}
lista_poder <- list(poder$power_median_dat)
```

Para ver el objeto, como siempre, solo tengo que llamarlo por el nombre que le haya dado (en este caso, `lista_poder`). Muestra el poder estadístico del meta-análisis para detectar diferentes tamaños de efecto (que llama `es`, por las siglas en inglés de *effect size*).

```{r}
lista_poder
```

Por ejemplo, nos muestra que para detectar un efecto de *z* = 0.1 (`es01`), se estima un poder de `0.1762492`; para un poder de *z* = 0.2 (`es02`), se estima un poder de `0.5356629`, y continúa así hasta un poder de  *z* = 1.0 (`es1`). 

Finalmente, nos muestra el poder estimado para detectar un efecto del tamaño del que estimamos como *real* (en nuestro caso, *z* = 0.15), bajo el encabezado `es_observed`. Como se ve, el poder es de tan solo `0.3364197`, lo que quiere decir que, si el efecto *real* de la asociación entre escrupulosidad y la adherencia a la medicación fuese *z* = 0.15, un meta-análisis como el nuestro sólo detectaría esa correlación como significativa en el 33.64% de los casos.

Lo que es más, podemos concluir que solo tenemos suficiente poder estadístico para detectar como significativos efectos desde aproximadamente *z* = 0.3 (`es03` = `0.8673657`, donde por fin tenemos un poder mayor a 0.8, o lo que es lo mismo, 80%). Sin embargo, nuestras estimaciones del efecto *real* están por debajo de este umbral, ya que en nuestro meta-análisis original fue de *z* = 0.15 y en el meta-análisis con pesos ajustados fue de *z* = 0.09.

#### Poder de cada estudio para diferentes tamaños de efecto.

El otro elemento generado es una base de datos modificada que, de manera similar a la lista de poder mediano, contiene el poder estimado para diferentes tamaños de efecto, pero en este caso no como una mediana, sino para cada estudio (`$dat`). Asignaré este elemento a un objeto que llamaré `poder_full_dat`.

```{r}
poder_full_dat <- poder$dat
poder_full_dat
```

Básicamente, esta tabla contiene información que nos permite evaluar si cada estudio incluido en nuestro meta-análisis tenía o no un poder estadístico suficiente para detectar un rango de tamaños de efecto que, como antes van desde *z* = 0.1 (columna `power_es01`), hasta *z* = 1.0 (`power_es1`). 

También, nos permite ver el poder estadístico de cada estudio para detectar un tamaño de efecto *real* como el que estimamos (*z* = 0.15), en la columna `power_es_observed` (la información de esta columna es análoga a la de los *funnel plot* creados con la función `viz_funnel` del paquete `metaviz`; Fig. \@ref(fig:funnel-plot3)A y B).

### Visualización del análisis de poder (*Firepower plot*) {#firepower-plot}

Estos análisis muestran que, en promedio, nuestro meta-análisis sólo alcanza un poder estadístico suficiente (mayor a 0.8 u 80%) para detectar de manera confiable efectos mayores a ~0.3, lo que está muy por encima de nuestras estimaciones del efecto real (0.15 en nuestro meta-análisis original, 0.09 en nuestro meta-análisis con pesos ajustados). De hecho, el poder estimado para detectar un efecto de 0.15 es de apenas 33.64% (reportado como `es_observed`, con un valor de `0.3364197` en el *output* 10 de la sección \@ref(poder-mediano)).

Puedo fácilmente crear una visualización de esto con la función `firepower` del paquete `metameta`. Solo tengo que agregar, como único argumento, el objeto en el que guardé la lista de poder mediano estimado para diferentes tamaños de efecto (que en este caso asigné a un objeto que llamé `lista_poder` en la sección \@ref(poder-mediano)). 

```{r}
poder.plot <- firepower(lista_poder)
```

En este caso asigné el *fireplot* resultante a un objeto, para poder modificar después, que llamé `poder.plot`. Entonces, para ver el resultado, debo correr el objeto (Fig. \@ref(fig:fire-plot1)).

```{r eval = FALSE}
poder.plot
```

```{r fire-plot1, fig.height = 2, echo = FALSE, fig.cap = "Fireplot básico de [metameta](https://www.dsquintana.blog/metameta-r-package-meta-analysis/), para un análisis de poder de nuestro meta-análisis. *Observed* hace referencia al tamaño de efecto observado en nuestro meta-análisis original; en este caso, 0.15."}
poder.plot$fp_plot
```

El objeto `poder.plot` contiene dos elementos: los datos (`$dat`) y la gráfica propiamente dicha (`$fp_plot`). Este último elemento es de clase `ggplot`, por lo que podemos usar funciones de `ggplot2` para cambiar, por ejemplo, el título del eje $X$ a "Tamaño de efecto", el título de la leyenda a "Poder", y el efecto observado de "*Observed*" a "Observado"[^13]. Por ejemplo:

[^13]: Para cambiar el título del eje $X$ usé la función `xlab`; para el título de la leyenda la función `guides` (opción `fill = guide_legend`); y para los valores del eje $X$, la función `scale_x_discrete`}.

```{r eval = FALSE}
poder.plot$fp_plot +
  xlab("Tamaño de efecto") +
  guides(fill = guide_legend(title = "Poder", 
                             reverse = TRUE)) +
  scale_x_discrete(labels = c("es_observed" = "Observado",
                              "es01" = 0.1, "es02" = 0.2, "es03" = 0.3, "es04" = 0.4, 
                              "es05" = 0.5, "es06" = 0.6, "es07" = 0.7, "es08" = 0.8, 
                              "es09" = 0.9, "es1"  = 1))
```

Este código produce la Figura \@ref(fig:fire-plot2), con todos los textos en español.

```{r fire-plot2, echo = FALSE, fig.height = 2, message = FALSE, fig.cap = "Fireplot básico de [metameta](https://www.dsquintana.blog/metameta-r-package-meta-analysis/), para un análisis de poder de nuestro meta-análisis, con el texto traducido a español y con la leyenda en una escala discreta para facilitar su lectura. *Observado* hace referencia al tamaño de efecto observado en nuestro meta-análisis original (en este caso, 0.15)."}
poder.plot$fp_plot +
  xlab("Tamaño de efecto") +
  guides(fill = guide_legend(title = "Poder", 
                             reverse = TRUE)) +
  scale_x_discrete(labels = c("es_observed" = "Observado",
                              "es01" = 0.1,    
                              "es02" = 0.2,
                              "es03" = 0.3,
                              "es04" = 0.4,    
                              "es05" = 0.5,    
                              "es06" = 0.6,    
                              "es07" = 0.7,
                              "es08" = 0.8,
                              "es09" = 0.9,
                              "es1"  = 1))
```

La Figura \@ref(fig:fire-plot2) nos permite comprobar, de nuevo, que el meta-análisis solamente alcanza un poder mínimo recomendado de 0.8 (80%) cuando se trata de efectos *z* = 0.3 (o mayores), y que para nuestro efecto *observado* (*z* = 0.15), el poder apenas alcanza cerca de 0.4 (40%).

Si el poder estadístico de nuestro meta-análisis para detectar ese efecto (*z* = 0.15) es demasiado bajo, por supuesto sería aún más deficiente para detectar un efecto menor, como el estimado en nuestro modelo con pesos corregidos (*z* = 0.09).

## Más información sobre influencia y meta-análisis combinatorio {#todas-combinaciones}

Como hemos visto, al realizar un meta-análisis, es fundamental tomar decisiones que pueden influir significativamente en los resultados y conclusiones. Una de las preguntas más importantes es qué estudios incluir en el análisis y si es necesario excluir algún estudio. Aunque nuestro diagnóstico inicial de influencia (sección \@ref(diag-inf)) no mostró la presencia de estudios atípicos en nuestro meta-análisis, en ocasiones se requiere un análisis más exhaustivo. 

En esta sección, que se basa por completo en la explicación y ejemplos de @quintanaOhMyGOSH2020, explicaré dos técnicas: *leave-one-out* (sección \@ref(leave-one-out)), que consiste en realizar una serie de meta-análisis excluyendo uno de los estudios incluidos en cada iteración, y meta-análisis combinatorio (sección \@ref(gosh)), que permite crear todas las combinaciones posibles de estudios y, por lo tanto, evaluar la robustez de los resultados en diferentes escenarios.

### *Leave-one-out* {#leave-one-out}

La técnica de *leave-one-out* realiza una serie de meta-análisis dejando fuera cada uno de los estudios incluidos. En nuestro caso, dado que tenemos 16 estudios, esta técnica ajustará 16 modelos (uno excluyendo cada estudio).

Esta técnica no sólo se puede hacer como parte de un diagnóstico de influencia (como se muestra en la sección \@ref(diag-inf)), sino que también se puede hacer para ver la estimación del efecto meta-analizado al excluir cada uno de los estudios de nuestra base de datos.

Hacerlo sólo requiere usar la función `leave1out` del paquete `metafor`, usando como argumento el resultado de nuestro meta-análisis original (en este caso, `res`):

```{r}
leave1out(res)
```

Como puedes ver, esta función crea una tabla con las columnas `estimate`, `se`, `zval`, `pval`, `ci.lb`, `ci.ub`, `Q`, `Qp`, `tau2`, `I2` y `H2` (explicadas en la sección \@ref(meta-interp)). Sin embargo, tenemos 16 filas, cada una de las cuales corresponde a los resultados cuando se excluye ese estudio del meta-análisis.

En este caso, los tamaños del efecto estimado (columna `estimate`) para los meta-análisis que eliminaron un único estudio están entre *z* = 0.14 y *z* = 0.16, lo que muestra que la eliminación de un estudio no influye mucho en el tamaño del efecto original estimado, que fue de *z* = 0.15. En otras palabras, podríamos eliminar cualquier estudio de nuestro análisis y el resultado cambiaría muy poco.

Aunque este análisis es bastante informativo, podría haber un subgrupo de estudios —en lugar de un único estudio— que influye en el resultado. Para investigar esto, podemos hacer un meta-análisis combinatorio.

### Meta-análisis combinatorio {#gosh}

Aunque la técnica de *leave-one-out* es útil para determinar la influencia de cada estudio en un meta-análisis, no nos dice nada de cuánto cambia nuestra estimación del efecto real cuando excluimos dos o más estudios. Por lo tanto, el meta-análisis combinatorio puede ser una técnica complementaria para detectar subgrupos de estudios que estén afectando nuestros resultados

Por ejemplo, si estamos realizando un meta-análisis sobre el efecto del consumo de café en la salud cardiovascular, es posible que encontremos varios estudios que no controlan adecuadamente el consumo de tabaco de los participantes. Si eliminamos uno solo de estos estudios, es posible que aún quede un sesgo importante en nuestra estimación del efecto *real*. En este caso, el meta-análisis combinatorio nos permitiría evaluar todas las posibles combinaciones de estudios que incluyen o excluyen los estudios que no controlan el consumo de tabaco, lo que nos ayudaría a comprender mejor el impacto de estos estudios en nuestra estimación del efecto *real*.

Un meta-análisis combinatorio ajusta una serie de meta-análisis a partir de todas las combinaciones posibles de estudios. Esta técnica es poderosa, pero es importante tener en cuenta que el número de posibles combinaciones aumenta exponencialmente a medida que aumenta el número de estudios incluidos en nuestro meta-análisis. De hecho, el número de análisis a realizar puede calcularse usando la fórmula $2^k - 1$, donde $k$ es el número de estudios meta-analizados (Tabla \@ref(tab:gosh-combs)). 

```{r gosh-combs, echo = FALSE}
tibble(k = 5:17,
       Combinaciones = 2^(k) - 1,
       k2 = 18:30,
       Combinaciones2 = 2^(k2) - 1) |> 
  mutate(Combinaciones = scales::number(Combinaciones),
         Combinaciones2 = scales::number(Combinaciones2)) |> 
  kable(linesep = "",
        booktabs = TRUE,
        align = c("c", "r", "c", "r"),
        col.names = linebreak(c("Número de\nestudios\n(\\textit{k})",
                                "Posibles\ncombinaciones\n($2^{k} - 1$)",
                                "Número de\nestudios\n(\\textit{k})",
                                "Posibles\ncombinaciones\n($2^{k} - 1$)"),
                              align = "c"),
        caption = "Número de posibles combinaciones según número de estudios (k)",
        escape = FALSE) |> 
  kable_styling(latex_options = c("HOLD_position")) |> 
  row_spec(0, align = "c")  |>
  column_spec(2, border_right = TRUE) |>
  column_spec(c(1,3), bold = TRUE)
```

Como se puede ver en la Tabla \@ref(tab:gosh-combs), el número de combinaciones aumenta drásticamente a medida que aumenta el número de estudios en nuestro meta-análisis. Por ejemplo, si tuviéramos 5 estudios, habría solo 31 posibles combinaciones ($2^{5} - 1$ = `r number(31)`). Sin embargo, si tuviéramos 20 estudios, habría más de un millón de combinaciones ($2^{16} - 1$ = `r number(1048575)`) y si tuviéramos 30 estudios, habría más de mil millones de combinaciones (ver Tabla \@ref(tab:gosh-combs)). Dado que esto puede consumir muchos recursos computacionales, es importante tener precaución al realizar un meta-análisis combinatorio con un gran número de estudios. El análisis puede tardar mucho tiempo e incluso fallar si se usa toda la memoria RAM del ordenador.

En nuestro ejemplo, estamos meta-analizando 16 estudios, lo que significa que necesitamos ajustar `r number(65535)` modelos ($2^{16} - 1$). Ajustar todos estos modelos puede llevar bastante tiempo, aunque esto dependerá de las especificaciones de tu equipo (en mi portátil, tarda alrededor de 5 minutos, pero ten en cuenta que en equipos viejos podría tardar 20, 30 minutos o incluso más).

Para visualizar la heterogeneidad entre los estudios, podemos utilizar una técnica llamada *visualización gráfica de la heterogeneidad de los estudios* (en inglés, *Graphical Display of Study Heterogeneity* o simplemente *GOSH*), propuesta por @olkinGOSHGraphicalDisplay2012. En R, podemos realizar esta visualización utilizando la función `gosh` del paquete `metafor`. Simplemente necesitamos proporcionar el resultado de nuestro meta-análisis original (`res`) como argumento. Opcionalmente, también podemos agregar el argumento `progbar = TRUE` para ver el progreso del análisis en la consola. Guardaré el resultado de este análisis en un objeto llamado `gp` para su posterior uso.

```{r cache = TRUE, message = FALSE}
gp <- gosh(res, progbar = TRUE)
```

Una vez que hayamos realizado el análisis *GOSH* podemos utilizar la función `plot` para visualizar los `r number(65535)` modelos. Simplemente necesitamos proporcionar el resultado de `gosh` (`gp` en este caso) como argumento para `plot`, pero en este ejemplo utilizaré los argumentos `breaks` y `labels` para personalizar la visualización. El argumento `breaks` nos permite definir el número de puntos de corte (y, por lo tanto, el número de barras) en los histogramas, mientras que el argumento `labels` nos permite cambiar los títulos de los ejes (en este caso para ponerlos en español). Esto nos permitirá crear una visualización más clara y fácil de entender de la heterogeneidad entre los estudios.

```{r gosh1, message = FALSE, fig.height = 3.7, fig.cap = "*GOSH plot* creado con la función  [`gosh`](https://www.metafor-project.org/doku.php/plots:gosh_plot) del paquete [`metafor`](https://www.metafor-project.org/doku.php). Un *GOSH plot* (en inglés, *Graphical Display of Study Heterogeneity*) es una visualización gráfica de la heterogeneidad de todos los posibles modelos de meta-análisis ajustados a partir de todas las combinaciones posibles de estudios. Muestra los efectos estimados para cada uno de los modelos en el eje horizontal, y su respectiva heterogeneidad en el eje vertical. La distribución de los efectos (parte superior de la gráfica) y la heterogeneidade (lado derecho de la gráfica) estan representadas como densidades e histogramas."}
plot(gp, breaks = 100,
     labels = c("Coeficiente de correlación (z de Fisher)", 
                expression(I^2)))
```

Los tamaños del efecto que se obtienen de los estudios incluidos en un meta-análisis de correlaciones varían ampliamente, generalmente entre *z* = 0.05 y *z* = 0.25. Esta variación subraya cómo la elección de los estudios incluidos en el meta-análisis puede influir en la estimación del tamaño del efecto. Sin embargo, no hay un subgrupo de resultados que se distinga significativamente del resto.

Para ilustrar mejor esto, voy a manipular manualmente el tamaño de efecto de uno de los estudios. Por ejemplo, en nuestro caso, podemos cambiar el tamaño del efecto del estudio 12, que inicialmente tenía un valor *z* de Fisher de 0.39, a un valor mucho mayor, de 0.70. Aunque esta base de datos alterada pierde validez para realizar cualquier análisis, nos permite visualizar el efecto de valores atípicos en una base de datos[^14].

[^14]: Normalmente identificamos formalmente estudios con resultados atípicos a partir de un diagnóstico de influencia (\@ref(diag-inf)) y técnicas tipo *leave-one-out* (sección \@ref(leave-one-out)), apoyándonos informalmente en el *forest plot* (sección \@ref(forest-inf)) y el *funnel plot* (sección \@ref(funnel-inf)).

Para evitar afectar nuestra base de datos original, creamos una copia de los datos en un objeto llamado `dat_mo`. Luego, utilizamos las funciones `mutate` y `replace` del paquete `dplyr` [@WickhamDplyr2021] para cambiar los tamaños de efecto del estudio 12 en la nueva base de datos.

```{r}
dat_mo <- dat |>
  mutate(yi = replace(yi, list = 12, values = 0.7))
```

Para visualizar cómo el estudio 12 se convierte en un valor atípico después de la manipulación de los datos, podemos realizar un meta-análisis de la nueva base de datos alterada (`dat_mo`) y representar los resultados en un diagrama de bosque o *forest plot*. Como se puede observar en la Figura \@ref(fig:for-plot-mo), el valor atípico introducido (estudio 12) es claramente visible en el *forest plot*.

```{r eval = FALSE}
res_mo <- rma.uni(yi, vi, data = dat_mo) 
forest(res_mo)
```

```{r for-plot-mo, echo = FALSE, fig.height = 4, fig.cap = "*Forest plot* básico de [metafor](https://www.metafor-project.org/doku.php) de la base de datos con valores atípicos introducidos manualmente para el estudio 12. Como se puede ver, este estudio tiene un tamaño de efecto muy diferente al de los demás estudios."}
res_mo <- rma.uni(yi, vi, data = dat_mo) 
par(mar = c(4,0,0,0))
forest(res_mo)
```

Hagamos el meta-análisis combinatorio con esta base de datos alterada que contiene el resultado atípico, usando de nuevo la función `plot` (Fig. \@ref(fig:gosh2)) para ilustrar el resultado.

```{r gosh2, cache = TRUE, message = FALSE, fig.height = 3.7, message = FALSE, fig.cap = "*GOSH plot* de un meta-análisis combinatorio que contiene un estudio con resultado atípico. En la parte superior del gráfico, se observa claramente un grupo de tamaños de efecto elevados con una alta heterogeneidad."}
gp_mo <- gosh(res_mo)
plot(gp_mo, breaks = 100,
     labels = c("Coeficiente de correlación (z de Fisher)", 
                expression(I^2)))
```

En la parte superior de la Figura \@ref(fig:gosh2) hay un claro grupo de tamaños de efecto relativamente grandes, con alta heterogeneidad. Podemos destacar específicamente los modelos del meta-análisis que incluyeron el estudio atípico con el argumento `out`:

```{r gosh3, fig.height = 3.7, fig.cap = "*GOSH plot* de un meta-análisis combinatorio que contiene un estudio con resultado atípico. En este caso, a diferencia de la Figura \\ref{fig:gosh2}, los resultados combinatorios que no contienen el estudio atípico están en azul, mientras que los meta-análisis que sí lo incluyen están en rojo. Los meta análisis que incluyen el estudio con un resultado atípico tienden a arrojar estimados más fuertes para la correlación, sesgando el resultado hacia valores más extremos, y son siempre altamente heterogéneos."}
plot(gp_mo, breaks = 100, out = 12,
     labels = c("Coeficiente de correlación (z de Fisher)", 
                expression(I^2)))
```

La inclusión del estudio 12 en los modelos de meta-análisis provoca un cambio notable en la distribución de las estimaciones del tamaño del efecto *real*. Este cambio indica la importancia de considerar cuidadosamente la inclusión de cada estudio en un meta-análisis, ya que la inclusión de valores atípicos —o estudios que no cumplan los criterios de calidad— puede tener un impacto significativo en los resultados.

En resumen, el meta-análisis combinatorio, junto con la visualización de datos mediante gráficos *GOSH*, permite demostrar cómo las opciones de análisis pueden influir en los resultados y conclusiones de un meta-análisis. Es fundamental realizar una selección cuidadosa de los estudios incluidos y llevar a cabo un análisis crítico y riguroso de los resultados obtenidos para garantizar la fiabilidad y validez del meta-análisis.

#### Identificación de estudios que contribuyen a los patrones de heterogeneidad a partir del *GOSH plot* {#fancy-gosh}

Una limitación del meta-análisis combinatorio (y el *GOSH plot*), es que no necesariamente nos permite identificar qué estudios están influyendo de manera preocupante en nuestro resultado. En el ejemplo pasado, sabíamos que el estudio 12 era el *culpable* pues así lo manipulamos. Sin embargo, al hacer un meta-análisis no necesariamente sabemos cuáles estudios son sospechosos. Por esto, tener herramientas que nos permitan identificar estudios potencialmente influyentes, es importante. 

Por ejemplo, la función `gosh.diagnostics` del paquete [`dmetar`](https://dmetar.protectlab.org/index.html) [@Harrer2019dmetar] se puede usar para crear análisis y visualizaciones muy completas a partir de los objetos generados con la función `gosh` del paquete `metafor`, como los que creamos en la sección \@ref(gosh), inmediatamente anterior [^15].

[^15]: El libro *Doing meta-analysis with R: a hands-on guide* [@harrer2021doing] se acompaña del paquete [`dmetar`](https://dmetar.protectlab.org/index.html) [@Harrer2019dmetar], y explica opciones para hacer meta-análisis tanto a partir de `metafor`, como a partir de otros paquetes (más información en el Apéndice \hyperlink{apendice-alt}{A}).

La función `gosh.diagnostics` utiliza tres algoritmos de aprendizaje automático (*machine learning*) para detectar patrones en nuestros datos: el algoritmo *k-means* [@hartiganAlgorithm136KMeans1979], la agrupación de densidad alcanzable y la conectividad, o *DBSCAN* [@schubertDBSCANRevisitedRevisited2017], y los modelos de mezcla gaussiana, o *GMM* [@fraleyModelBasedClusteringDiscriminant2002].

Básicamente, usa esos tres métodos para buscar grupos de resultados (o *clusters*) entre todas las combinaciones posibles de estudios que creamos con la función `gosh`. Con esto, y como explicaré en esta sección, ayuda a identificar estudios influyentes en nuestro resultado a partir de una identificación de posibles valores atípicos.

Es importante tener en cuenta que, al menos por ahora, y tal como sucede con `metameta`, el paquete `dmetar` debe instalarse desde GitHub:

```{r eval = FALSE}
#se debe tener instalado el paquete devtools
devtools::install_github("MathiasHarrer/dmetar")
```

Una vez hemos instalado `dmetar`, podemos cargar el paquete.

```{r message = FALSE}
library(dmetar)
```

Como ejemplo, voy a usar el objeto `gp_mo` que creamos antes, a partir de la base de datos alterada que contiene el resultado atípico (estudio 12). Voy a usar la función `gosh.diagnostics` usando ese objeto como argumento.

Sin embargo, es posible agregar parámetros a cada algoritmo como argumentos. Si quieres información detallada y más profunda, puedes ver la [documentación](https://dmetar.protectlab.org/reference/gosh.diagnostics) de la función `gosh.diagnostics`, y los ejemplos en el capítulo 5 del libro *Doing Meta-Analysis in R: A Hands-on Guide* [@harrer2021doing]. 

En este caso, voy a agregar el argumento `km.params = list(centers = 2)` para especificar que el algoritmo *k-means* debe buscar dos *clusters* (`centers`) en nuestros datos (esto no es estrictamente necesario, pero de manera automática el algoritmo buscará tres; decidí poner dos, pues visiblemente hay dos grupos de resultados, como se ve en la Fig. \@ref(fig:gosh2) y especialmente en la Fig. \@ref(fig:gosh3)). 

Adicionalmente, incluiré el argumento `db.params = list(eps = 0.1, MinPts = 50)`, para especificar parámetros para el algoritmo *DBSCAN*, pues los *clusters* requieren un número mínimo de puntos (`MinPts`: *minimum number of points* que se refiere mínimo número de puntos por *cluster*) dentro de una distancia máxima (`eps`: $\epsilon$ o épsilon) alrededor de uno de sus miembros (o "semilla")[^16]. En este caso, agregué `eps = 0.1` para especificar que la "distancia de *alcanzabilidad*" (en inglés, *reachability distance*) es de 0.1, y `MinPts = 50` para determinar el número mínimo de puntos en cada *cluster* (en este caso, 50). 

[^16]: Elegir los parámetros `eps` y `MinPts` no es necesariamente una tarea sencilla. No hay una única forma de elegir estos parámetros, ni una manera automática y sencilla de hacerlo. Las técnicas existentes son relativamente complejas, por lo que no las incluyo acá. Si quieres información detallada sobre estos parámetros, te recomiendo leer @esterAlgorithm y @schubertDBSCANRevisitedRevisited2017, así como buscar diferentes maneras prácticas de elegir los valores para estos parámetros propuestas en *blogs* como [este](https://www.datanovia.com/en/lessons/dbscan-density-based-clustering-essentials/) en inglés, o [este](http://exponentis.es/parametrizacion-automatica-de-dbscan-en-r-a-partir-de-la-curva-elbow) en español.

En este caso, asignaré el resultado a un objeto que llamaré `gp_diag`.

```{r eval = FALSE}
gp_mo_diag <- gosh.diagnostics(gp_mo,
                               km.params = list(centers = 2),
                               db.params = list(eps = 0.1, MinPts = 50))
```

De manera conveniente, en la consola se puede ver el progreso de la función.

```{r cahe = TRUE, echo = FALSE, message = FALSE}
gp_mo_diag <- gosh.diagnostics(gp_mo,
                               km.params = list(centers = 2),
                               db.params = list(eps = 0.1,
                                                MinPts = 50))
```

Para ver un resumen, solo tengo que usar la función `summary` agregando como argumento el objeto generado con la función `gosh.diagnostics` (`gp_mo_diag`):

```{r}
summary(gp_mo_diag)
```

En el *output*, vemos el número de *clusters* que ha detectado cada algoritmo. En este caso, el modelo *K-means* detectó 2 *clusters*, *DBSCAN* detectó 3, y *GMM* detectó 9.  Dado que cada enfoque utiliza una estrategia matemática diferente para segmentar los datos, es normal que el número de *clusters* no sea el mismo.

De manera importante, el *output* también nos reporta explícitamente los estudios identificados como potencialmente atípicos, bajo el encabezado "`Identification of potential outliers`". En este caso, mientras que el modelo *K-means* sugiere que el estudio 12 es atípico (e influyente), los modelos *DBSCAN* y *GMM* detectaron tanto el estudio 12, como el estudio 10 como potencialmente atípicos. En otras palabras, los tres modelos coinciden en señalar el estudio 12 como atípico.

Para ver esto gráficamente, puedo sencillamente usar la función `plot` incluyendo como argumento el objeto que creamos con la función `gosh.diagnostics` (en este caso, `gp_mo_diag`). 

```{r  eval = FALSE}
plot(gp_mo_diag)
```

Esto creará una serie de figuras; primero, una figura por cada algoritmo (*K-means*, *DBSCAN* y *GMM*), y después una figura por cada estudio potencialmente atípico que se haya identificado (en nuestro caso, los estudios 12 y 10; si no se identificara ninguno, estas gráficas no se harían). Si estás usando RStudio, deberás moverte entre las figuras usando las flechas que están sobre la pestaña *Plots* del panel donde aparecen, pues sólo será visible la última figura. 

En este caso, voy a producir cada figura independientemente para analizarla.

Primero, tenemos la Figura \@ref(fig:fancy-gosh1) que muestra los resultados del algoritmo *K-means*. Incluye 3 paneles, llamados *K-means Algorithm*, que es un *GOSH plot* pero resaltando en colores las combinaciones de estudios asignadas a cada *cluster* (izquierda); *Cluster imbalance (K-means algorithm)*, que representa el porcentaje de cambio entre los *clusters* producto de cada estudio (derecha, arriba); y *Cluster imbalance (Cook's Distance)*, que nos muestra la distancia de Cook producto de cada estudio para cada *cluster* (derecha, abajo).

```{r fancy-gosh1, echo = FALSE, out.height = "170px", fig.align = "center", fig.cap = "\\textit{GOSH plot}, desequilibrio de \\textit{clusters} y distancia de Cook estimados usando el algoritmo \\textit{K-means}. Los \\textit{clusters} identificados se representan con diferentes colores, y sus efectos se muestran en los 3 paneles. Este algoritmo identificó 2 \\textit{clusters}, pues así fue especificado."}
knitr::include_graphics("images/fancy_gosh_P1.pdf")
```

¿Qué nos muestra la Figura \@ref(fig:fancy-gosh1)? Primero, en el *GOSH plot* podemos ver que los dos *clusters* coinciden casi perfectamente con los que encontramos en la Figura \ref{fig:gosh2}, cuando especificamos explícitamente que se debían agrupar las combinaciones que incluían el estudio 12. Esto es importante pues acá no especificamos qué estudios son sospechosos, y el algoritmo mismo está identificando un *cluster* que sabemos, contiene un estudio con un resultado atípico e influyente.

Pero, de manera aún más clara, vemos que hay un gran desbalance entre *clusters* al incluir el estudio 12.  Si medimos el desbalance entre *clusters* como porcentaje de cambio, podemos ver que los dos *clusters* se alejan muchísimo en el estudio 12; si medimos el desbalance como distancia de Cook, podemos ver que hay un pico muy importante para ambos *clusters* en el estudio 12, lo que quiere decir que ese estudio influye muchísimo en la distancia entre esos dos *clusters*. 

Luego, tenemos la Figura \@ref(fig:fancy-gosh2) que muestra los resultados del algoritmo *DBSCAN*. Incluye también 3 paneles, equivalentes a los de la Figura \@ref(fig:fancy-gosh1), pero acá llamados *DBSCAN Algorithm (black dots are outliers)*, *Cluster imbalance (Density-Based Clustering)*, y al igual que en la Figura \@ref(fig:fancy-gosh1), *Cluster imbalance (Cook's Distance)*.

```{r fancy-gosh2, echo = FALSE, out.height = "170px", fig.align = "center", fig.cap = "\\textit{GOSH plot}, desequilibrio de \\textit{clusters} y distancia de Cook estimados usando el algoritmo \\textit{DBSCAN}. Los \\textit{clusters} identificados se representan con diferentes colores, y sus efectos se muestran en los 3 paneles. En el \\textit{GOSH plot}, las combinaciones de estudios que no pudieron ser asignadas a ningún \\textit{cluster} están en negro. Este algoritmo identificó 3 \\textit{clusters}."}
knitr::include_graphics("images/fancy_gosh_P2.pdf")
```

En el *GOSH plot* de la Figura \@ref(fig:fancy-gosh2) muestra los tres clusters detectados por el algoritmo *DBSCAN* (verde, azul claro y morado), así como las combinaciones de estudios que —dados los parámetros `eps` y `MinPts` que usamos en la función `gosh.diagnostics`— no fueron clasificadas en ningún *cluster* (*Outlier*, en negro). El *cluster* 2 (azul claro), corresponde en buena medida al *cluster* 2 de la Figura \@ref(fig:fancy-gosh1), que a su vez coincide ampliamente con la Figura \ref{fig:gosh2}.

Adicionalmente, el desbalance entre *clusters* muestra picos para el estudio 12, tanto cuando es medido como porcentaje de cambio, como cuando es medido como distancia de Cook. Sin embargo, acá también encontramos un pico de menor tamaño para el Estudio 10, que explica por qué fue señalado como potencialmente atípico.

Finalmente, tenemos la Figura \@ref(fig:fancy-gosh3) que muestra los resultados del algoritmo *GMM*. Incluye de nuevo tres paneles equivalentes, llamados *Gaussian Mixture Model*, *Cluster imbalance (GMM)*, y al igual que en la Figuras \@ref(fig:fancy-gosh1) y \@ref(fig:fancy-gosh2), *Cluster imbalance (Cook's Distance)*.

```{r fancy-gosh3, echo = FALSE, out.height = "170px", fig.align = "center", fig.cap = "\\textit{GOSH plot}, desequilibrio de \\textit{clusters} y distancia de Cook estimados usando el algoritmo de modelos de mezcla gaussiana (en inglés, \\textit{Gaussian Mixture Model} o \\textit{GMM} por sus siglas). Los \\textit{clusters} identificados se representan con diferentes colores, y sus efectos se muestran en los 3 paneles. Este algoritmo identificó 9 \\textit{clusters}."}
knitr::include_graphics("images/fancy_gosh_P3.pdf")
```

El *GOSH plot* de la Figura \@ref(fig:fancy-gosh3) muestra los nueve clusters detectados por el algoritmo *GMM*. Aunque en este caso no hay ningún *cluster* que corresponda tan claramente con el *cluster* 2 de las Figuras \@ref(fig:fancy-gosh1) o \@ref(fig:fancy-gosh2), o con la Figura \ref{fig:gosh2}, sí vemos de nuevo picos muy importantes para el estudio 12, especialmente cuando el desbalance entre *clusters* es medido como distancia de Cook. En últimas, los tres algoritmos coinciden en señalar que el estudio 12 es potencialmente atípico y podría tener demasiada influencia en nuestra estimación del efecto *real*.

Como mencioné antes, la función `plot` también crea una figura por cada estudio potencialmente atípico que se haya identificado (en nuestro caso, los estudios 12 y 10; Fig \ref{fig:fancy-gosh4}).

```{r fancy-gosh4, echo = FALSE, out.height = "150px", fig.align = "center", fig.cap = "\\textit{GOSH plot} de cada estudio identificado como potencialmente atípico por los algoritmos de la función \\texttt{gosh.diagnostics}. En este caso, los estudios 12 (izquiera) y 10 (derecha). En azul claro se resaltan los resultados de las combinaciones de estudios que incluyen cada uno de esos estudios potencialmente atípicos."}
knitr::include_graphics("images/fancy_gosh_P4.pdf")
```

Como verás, la Figura \ref{fig:fancy-gosh4} es simplemente una serie *GOSH plots*, señalando en azul claro las combinaciones de estudios que incluyen cada uno de los estudios identificados como potencialmente atípicos: el estudio 12 (izquierda) y el estudio 10 (derecha). En este caso, por supuesto, la figura que resalta las combinaciones que incluyen el estudio 12 corresponde completamente a lo mostrado en la Figura \ref{fig:gosh2}.

En conjunto, estas técnicas incluidas en la función `gosh.diagnostics` nos permiten identificar estudios potencialmente atípicos, incluso cuando no tenemos indicación o sospecha de algún o algunos estudios. En este caso, la función indicó dos estudios como potencialmente atípicos (estudios 12 y 10), y los tres algoritmos, a pesar de ser distintos, coincidieron en señalar especialmente al estudio 12 como influyente (algo que en este caso sabemos que es verdad, pues manipulamos su resultado).

---------------

# Meta-análisis de correlación con moderador (meta-regresión) {#met-moderation}

La inclusión de moderadores en un meta-análisis mediante una técnica llamada meta-regresión, combina principios de regresión meta-analítica y lineal. Esta técnica nos permite determinar si existe una asociación lineal entre el resultado de nuestro meta-análisis y una o más covariables; es decir, variables que pueden estar influyendo en los resultados de los estudios que estamos analizando. Al incluir estos moderadores en nuestro análisis, podemos controlar mejor la heterogeneidad y entender cómo factores específicos pueden estar afectando los resultados del meta-análisis.

Por ejemplo, en el caso de la relación entre la altura de una planta y la cantidad de luz que recibe, el tipo de suelo puede actuar como variable moderadora. Es decir, el efecto de la cantidad de luz en la altura de la planta puede ser diferente dependiendo del tipo de suelo, y estudios hechos en diferentes tipos de suelo podrían tender a encontrar resultados distintos. Por lo tanto, en este caso, el tipo de suelo actuaría como una variable moderadora de la relación entre la cantidad de luz y la altura de la planta, que deberíamos tener en cuenta en cualquier meta-análisis.

En nuestro caso, variables como la cantidad de variables controladas (`controls`), el diseño de cada estudio (`design`), la forma en la que se midieron la adherencia al tratamiento (`a_measure`) y la escrupulosidad (`c_measure`), la edad de los participantes (`meanage`) y/o la calidad metodológica de los estudios (`quality`), son características que podrían haber afectado el coeficiente de correlación que encontraron (todas estas variables se explican con mayor detalle en la sección \@ref(variables-inf)). Por ejemplo, estudios con menor calidad metodológica podrían haber tendido a sobrestimar el efecto, encontrando correlaciones más fuertes y positivas entre escrupulosidad y adherencia al tratamiento, sesgando así nuestra estimación del efecto *real*.

Los moderadores pueden ser tanto variables continuas, como categóricas, y en las siguientes secciones daré ejemplos para cada caso.

## Ejemplo 1: Moderación de una variable continua (edad promedio de los participantes) {#ex-mod1}

Primero, y como ejemplo, vamos a ver si la edad promedio de los participantes de un estudio (en nuestros datos, `meanage`) modera el resultado. Esto es importante, pues hay una enorme variación entre las edades medias de los participantes de los diferentes estudios[^17], lo que podría moderar (afectar) la asociación entre escrupulosidad (*conscientiousness*) y adherencia a la medicación prescrita.

[^17]: De hecho, mientras que en el estudio de `r paste0(dat$authors[which.min(dat$meanage)], " (", dat$year[which.min(dat$meanage)], ")")` la edad promedio fue de `r min(dat$meanage)`, en el estudio de `r paste0(dat$authors[which.max(dat$meanage)], " (", dat$year[which.max(dat$meanage)], ")")` la edad promedio fue de `r max(dat$meanage)`.

Para esto, de nuevo podemos usar la función `rma` de paquete `metafor` de la misma manera que en la sección \@ref(meta-cor), pero agregando nuestra variable moderadora (`meanage`) al argumento `mods`. En este caso voy a asignar a un objeto llamado `res.modage`, para diferenciarlo del objeto `res` al que asigné el meta-análisis básico, sin moderadores.

```{r}
res.modage <- rma(yi = yi, vi = vi, 
                  mods = ~meanage, 
                  data = dat)
```

Los resultados, son los siguientes:

```{r}
res.modage
```

Estos resultados tienen la misma organización que los del análisis sin moderadores (sección \@ref(meta-cor)). Este resultado nos muestra que, a pesar de la gran diferencia de edad entre estudios, la edad no tiene un efecto significativo, como se puede ver en la sección "`Test of Moderators (coefficient 2)`" (al final nos muestra el valor *p* como "\texttt{p-val =  0.2320}"), así como los resultados para el efecto de `meanage` en la tabla `Model Results` (donde nos da el mismo resultado: "\texttt{0.2320}").

En la tabla de `Model results` los estimados (columna `estimate`) se pueden leer tal cual como si fueran una regresión, pero teniendo en cuenta que acá nuestra variable dependiente sería la correlación entre escrupulosidad y adherencia al tratamiento (en valores *z*). Entonces, como el intercepto (`intrcpt`) tiene un valor estimado de 0.2741, para estudios con participantes que tuviesen una edad media de 0 años, se estimaría que la asociación entre escrupulosidad y adherencia al tratamiento sería de *z* = 0.2741. Por su parte el efecto de la edad media (`meanage`), nuestro moderador, es de -0.0024, lo que indica que por cada año de aumento en la edad promedio de los participantes, la correlación entre escrupulosidad y adherencia disminuye *z* = -0.0024. Para decirlo de manera sencilla, en nuestra muestra de estudios, la correlación entre adherencia y escrupulosidad tiende a disminuir con la edad, pero esta tendencia no es significativa.

### Más información e interpretación de la moderación {#pred-mods}

Para más información, podemos *predecir* el efecto (en este caso, la correlación entre la escrupulosidad y la adherencia a la medicación), a diferentes edades, usando la función `predict`. 

En este ejemplo, como argumentos de esta función incluiré el objeto que contiene el meta-análisis con la edad como variable moderadora (`res.modage`), y los los valores para los cuales quiero saber el coeficiente de correlación predicho (argumento `newmods`); en este ejemplo voy a usar "`seq(20, 80, by = 10)`", lo que produce la secuencia de números de 20 a 80, cada 10 (es decir: 20, 30, 40, 50, 60, 70, 80), que serán las edades para las cuales obtendré el coeficiente de correlación estimado por el modelo[^18].

[^18]: Escogí edades entre 20 y 80 como referencia, pues están cerca al rango de valores de de las edades promedio de los estudios meta-analizados (`r min(dat$meanage)` a `r max(dat$meanage)`), y con edades cada 10 años, para obtener un número manejable pero informativo de predicciones.

Adicionalmente, voy a convertir esta tabla a un objeto clase `data.frame` (usando la función `as.data.frame()`), crearé una nueva variable llamada `meanage` que contiene las edades para las cuales he calculado la predicción (`mutate(meanageage = seq(20, 80, by = 10))`[^19]), y voy a reorganizar el orden de las columnas (`select(7, 1:6)`). Asignaré esta tabla a un objeto llamado `pred.res.modage`.

[^19]: Para agregar los valores de la variable moderadora para los cuales se hace la predicción, alternativamente se puede agregar el argumento `addx = TRUE` a la función `predict` (en este caso, por ejemplo, `predict(res.modage, newmods = seq(20, 80, by = 10), addx = TRUE)`). Sin embargo, para evitar crear columnas adicionales y tener una tabla en un formato más sencillo y claro, he decidido crear la columna `meanage` manualmente. Si usara el argumento `addx = TRUE`, crearía las columnas `X.intrcpt` con el intercepto, y `X.meanage` con los valores de `meanage`.

```{r pred-mod1}
# Calcular efecto ajustado para diferentes edades
pred.res.modage <- predict(res.modage, newmods = seq(20, 80, by = 10)) |> 
  as.data.frame() |> 
  mutate_all(~round(., 3)) |> 
  mutate(meanage = seq(20, 80, by = 10)) |> 
  select(7, 1:6)
# Ver la tabla
pred.res.modage
```

Con esto, el objeto `pred.res.modage`, al que he asignado esta predicción, tiene las siguientes columnas:

-   **`meanage`**: edad promedio de los participantes de los estudios meta-analizados

-   **`pred`**: valor del efecto predicho (en este caso, coeficiente de correlación transformado en *z* de Fisher), para cada edad promedio

-   **`se`**: error estándar (en inglés *standard error*) del efecto predicho

-   **`ci.lb`**: límite inferior del intervalo de confianza del 95% (en inglés *confidence interval lower bound*) del efecto predicho

-   **`ci.ub`**: límite superior del intervalo de confianza del 95% (en inglés *confidence interval upper bound*) del efecto predicho

-   **`pi.lb`**: límite inferior del intervalo de predicción (en inglés *prediction interval lower bound*), y

-   **`pi.ub`**: límite superior del intervalo de predicción (en inglés *prediction interval upper bound*)

Las columnas `age` (edad) y `pred` (efecto predicho), nos muestran que, para estudios donde los participantes son en promedio más jóvenes, se esperaría una correlación más fuerte; por ejemplo, mientras que para estudios con personas de `r pred.res.modage[1,1]` años de edad en promedio se esperaría una correlación de *z* = `r round(pred.res.modage[1,2], 3)`, para estudios con participantes con una edad promedio de `r pred.res.modage[dim(pred.res.modage)[1],1]` años, el modelo estima un efecto de *z* = `r round(pred.res.modage[dim(pred.res.modage)[1],2], 3)`. En otras palabras, en nuestra muestra de estudios, la asociación entre escrupulosidad y adherencia a la medicación tiende a reducirse ligeramente con la edad promedio de la muestra. 

Sin embargo, este efecto moderador de la edad \underline{no es significativo}, por lo que no tenemos evidencia suficiente para inferir que se da más allá de nuestra muestra de estudios.

#### *Meta-Analytic Scatter Plot* (Gráfico de dispersión meta-analítico) {#meta-scatter}

Alternativamente, cuando tenemos un modelo con moderador, también podemos ver la asociación entre la variable moderadora, y el efecto de cada estudio meta-analizado, a modo de regresión. La función `regplot` hace precisamente esto (Fig. \@ref(fig:reg-plot1)).

```{r reg-plot1, fig.height = 4.8, fig.cap = "Gráfico de dispersión meta-analítico (*Meta-Analytic Scatter Plot*). El tamaño de los puntos es proporcional al peso que recibieron los estudios en el meta-análisis (puntos más grandes para los estudios con más peso, pues tienen un tamaño de muestra mayor y con un menor error estimado). La línea negra representa el efecto previsto en función del predictor (en este caso \\texttt{meanage}, edad promedio), que por supuesto coincide  con las predicciones del objeto `pred.res.modage` (Output 16, en la sección \\@ref(pred-mods)); la banda gris delimitada por líneas punteadas representa el intervalo de confianza del 95%."}
regplot(res.modage,
        ylab = "Coeficiente de correlación (z de Fisher)",
        xlab = "Edad promedio de muestra")
```

### *Forest plot* y *funnel plot* {#plot-mod}

Por supuesto, de estos resultados también puedo crear *forest plots* y *funnel plots*, siguiendo los ejemplos y código de la sección \@ref(meta-cor). Para el *forest plot*, hago a continuación un ejemplo anotado y mejorado[^20] (Fig. \@ref(fig:for-plot-mod1), con un código similar al usado como ejemplo en la Fig. \@ref(fig:for-plot2)). Sin embargo, es importante tener en cuenta que esta opción no creará un resumen del meta-análisis, ya que no tenemos un solo efecto *real* estimado como producto del meta-análisis.

[^20]: La función [`viz_forest`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html#creating-forest-plots-with-function-viz_forest) del paquete [`metaviz`](https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html) no puede crear un *forest plot* de un meta-análisis ajustado con `metafor` cuando este modelo contiene variables moderadoras continuas, o cuando tiene como variable moderadora más de una variable categórica. En otras palabras, sólo podrá hacer el *forest plot* si nuestro meta-análisis no tiene moderadores, o tiene un único moderador categórico. Como en este ejemplo nuestro moderador es una variable continua, no es posible usar esta función.

```{r eval = FALSE}
# forest plot con anotaciones adicionales
forest(res.modage,  cex = 0.75, xlim = c(-1.6, 1.6),
       slab = paste(dat$authors, dat$year, sep = ", "),
       showweights = TRUE,
       xlab = "Coeficiente de correlación (z de Fisher)",
       digits = c(2,3L))
# agregar encabezados a las columnas (valores de X y Y deben ser ajustados)
par(cex = 0.8, font = 2)
text(x = -1.6, y = 18, labels = "Autor(es), Año", pos = 4)
text(x = 0, y = 18, labels = "Efecto e IC", pos = 4)
text(x = 1, y = 18, labels = "Peso", pos = 2)
text(x = 1.6, y = 18, labels = "Corr. [95% IC]", pos = 2)
```

```{r for-plot-mod1, echo = FALSE, warning = FALSE, fig.height = 3.5, fig.cap = "*Forest plot* básico de [metafor](https://www.metafor-project.org/doku.php), para un meta-análisis incluyendo la edad promedio de los participantes como moderador. En la ilustración gráfica, además de los efectos originales, se puede ver el efecto de cada estudio estimado cuando se incluye el moderador como polígonos (diamantes) de color gris. Sin embargo, ya no obtenemos una fila al final representando el efecto promediado del meta-análisis, ya que no tenemos un solo efecto."}
# forest plot con anotaciones adicionales
par(mar = c(4,0,0,0))
forest(res.modage,  cex = 0.75, xlim = c(-1.6, 1.6),
       slab = paste(dat$authors, dat$year, sep = ", "),
       showweights = TRUE,
       xlab = "Coeficiente de correlación (z de Fisher)",
       digits = c(2,3L))
# agregar encabezados a las columnas (valores de X y Y deben ser ajustados)
par(cex = 0.8, font = 2)
text(x = -1.6, y = 18, labels = "Autor(es), Año", pos = 4)
text(x = 0, y = 18, labels = "Efecto e IC", pos = 4)
text(x = 1, y = 18, labels = "Peso", pos = 2)
text(x = 1.6, y = 18, labels = "Corr. [95% IC]", pos = 2)
```

De manera similar, podemos obtener un *funnel plot* de nuestro meta-análisis con moderador. Sin embargo, tanto el paquete `weightr` como el paquete `metafor` tienen funciones llamadas `funnel`, lo que puede crear un conflicto. Para evitar esto, podemos explícitamente pedir a R que use la función `funnel` del paquete `metafor` con el comando `metafor::funnel`:

```{r eval = FALSE}
metafor::funnel(res.modage, xlab = "Valor residual", ylab = "Error estándar")
```

```{r funnel-plot-mod1, echo = FALSE, fig.height = 3.5, fig.cap = "*Funnel plot* básico de [metafor](https://www.metafor-project.org/doku.php), para un meta-análisis incluyendo la edad promedio de los participantes como moderador, y con títulos de los ejes en español. La línea punteada vertical representa el efecto meta-analizado que hemos encontrado, así que podemos ver los estudios que encontraron un efecto mayor (derecha de la línea punteada) o menor (izquierda) de éste."}
par(mar = c(4,4,0,1))
metafor::funnel(res.modage,
                xlab = "Valor residual",
                ylab = "Error estándar")
```

Este *funnel plot* (Fig. \@ref(fig:funnel-plot1)), a diferencia los los anteriores, nos muestra los valores residuales de cada estudio en vez de los coeficientes de correlación (transformados a $z$ de Fisher). Es decir, nos muestra qué tanto se aleja cada estudio del resultado de nuestro meta-análisis, que tiene un valor residual de 0 (pues no se aleja de sí mismo).

### Reporte del meta-análisis con moderador continuo {#reporte-cont}

Por supuesto, al igual que como se explica en la sección \@ref(heterog-inf), también podemos estimar intervalos de confianza de las medidas de heterogeneidad, con la función `confint(res.modage)`. Incluyendo esos intervalos de confianza el modelo se podría resumir, por ejemplo, así:

```{=latex}
\begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
  colback=iacol!5!white,colframe=iacol!75!white,colbacktitle=iacol,
  title=Ejemplo de reporte con moderador continuo,fonttitle=\bfseries, parbox=false,
  boxed title style={size=small,colframe=iacol} ]
  
Dado que hay una considerable variación en la edad media de los participantes entre los estudios incluídos (con un rango entre 22 y 78.6), volvimos a ajustar el mismo meta-análisis, pero incluyendo la edad media de los participantes de cada estudio como variable moderadora. Esta meta-regresión reveló que, aunque en los estudios icluídos la asociación entre escrupulosidad y adherencia disminuye al aumentar la edad media de las muestras, esta moderación no es significativa (intercepto: $z$ ± $se$ = 0.27 ± 0.11, IC 95\% [0.06, 0.49]; $Z$ = 2.51, $p$ = .012; edad promedio: $z$ ± $se$ = -0.002 ± 0.002, IC 95\% [-0.006, 0.002]; $Z$ = -1.195, $Q$(1) = 1.429,   $p$ = .232). 

Adicionalmente, la heterogeneidad moderada y significatica se mantuvo, incluso al incluir este moderador ($\tau^2$ ± $se$ =  0.0072 ± 0.0054, IC 95\% [0.0010, 0.0403]; $\tau$ = 0.0846, IC 95\% [0.0315, 0.2007]; $Q$(14) =  30.91, $p$ = .006; $I^2$ = 56.50\%, IC 95\% [15.24\%, 87.97\%]). Por esto, no podemos inferir que la edad promedio de las muestras tenga un efecto en la estimación de la asociación entre escrupulosidad y adherencia al tratamiento.  

\end{tcolorbox}
```

Por supuesto, este resultado está pensado acá para ser reportado tras el reporte del meta-análisis básico (ejemplos en las secciones \@ref(reporte1) y \@ref(reporte2)).

De nuevo, ten en cuenta que siempre es buena idea citar tablas y figuras relevantes junto al reporte de los resultados que ilustran, pues esto facilitará la comunicación de resultados complejos. En este caso, yo citaría el gráfico de dispersión meta-analítico (*Meta-Analytic Scatter Plot*; sección \@ref(meta-scatter)) al presentar los resultados de la edad promedio, y el *forest plot* y el *funnel plot* de este meta-análisis con moderador (sección \@ref(plot-mod)) al hablar de heterogeneidad.

Aunque estoy siendo extremadamente exhaustivo en la manera de reportar toda la información, hay diferentes formas de reportar tus resultados. Puedes por supuesto usar ejemplos de artículos que reporten meta-análisis para usarlos como modelo; por ejemplo, @salsmanMetaanalyticApproachExamining2015 y @shiCorrelationAdherenceRates2010 hacen reportes distintos pero claros, y el estudio de  @hidalgo-fuentesUsoProblematicoInternet2022 es un ejemplo sencillo del reporte de un meta-análisis de correlación, con y sin moderador, en español.

## Ejemplo 2: Moderación de una variable categórica (controles usados en cada estudio meta-analizado)

Como segundo ejemplo, voy a mirar si el hecho de que los estudios tengan variables que fueron controladas, modera la asociación entre escrupulosidad (*conscientiousness*) y adherencia a la medicación prescrita. Siguiendo los mismos pasos, voy hacer este análisis, pero voy a asignar este meta-análisis a un objeto llamado `res.contr`. 

```{r}
res.contr <- rma(yi = yi, vi = vi, mods = ~controls, data = dat)
res.contr
```

En este caso, a diferencia del ejemplo de moderación anteriores, la variable moderadora (`controls`) sí tiene un efecto significativo, como se puede ver en el "`Test of Moderators (coefficient 2)`" y en la columna `pval` para el efecto de `controlsmultiple` (<.0001), así como en los asteriscos que aparecen al final de esa fila (`***`).

De nuevo, en la tabla de `Model results` los estimados (columna `estimate`) se pueden leer tal cual como si fueran una regresión. Sin embargo es importante tener en cuenta que, dado que nuestra variable moderadora es categórica, el intercepto acá será en valor estimado de la correlación para el primer nivel (o categoría) de nuestra variable moderadora. Si recuerdas, como se explica en la sección \@ref(variables-inf) la variable que estamos usando como moderadora, `controls` contiene información de si cada estudio no controló ninguna variable (`none`), o controló múltiples variables (`multiple`). Es decir, es una variable categórica con 2 niveles (`none`, `multiple`). 

Entonces, como el intercepto (`intrcpt`) tiene un valor estimado de 0.1788, se estima que la asociación entre escrupulosidad y adherencia al tratamiento fue de *z* = 0.1788 para estudios que no controlaron ninguna variable. Después, frente al efecto de nuestro moderador (`controlsmultiple`, que pone de manera conjunta la variable `controls` más el nivel `multiple`), nos da un estimado de -0.1621, lo que quiere decir que la correlación disminuye *z* = -0.1621 cuando se trata de estudios que controlaron múltiples variables, \underline{con relación al intercepto} (`none`, en nuestro caso). De manera concreta, el modelo estimó una correlación de *z* = 0.1788 para estudios que no controlaron ninguna variable, y una correlación de *z* = 0.0167 (o, lo que es lo mismo, 0.1788 - 0.1621) cuando los estudios usaron varios controles.

### Más información e interpretación de la moderación {#pred-mods2}

Al igual que en el ejemplo con un moderador continuo (sección \@ref(pred-mods)), puedo *predecir* el efecto (en este caso, la correlación entre la escrupulosidad y la adherencia a la medicación), cuando hay o no controles, usando la función `predict`.

```{r}
pred.res.contr <- predict(res.contr, newmods = c(0, 1)) |> 
  as.data.frame() |> 
  mutate_all(~round(., 3)) |>
  mutate(controls = levels(dat$controls)) |> 
  rename(yi = pred) |> 
  select(7, 1:6)
pred.res.contr
```

En este ejemplo, como argumentos de esta función incluí el objeto que contiene el meta-análisis con la edad como variable moderadora (`res.contr`), y los los valores para los cuales quiero saber el coeficiente de correlación predicho (argumento `newmods`); dado que la variable moderadora es categórica, el modelo genera variables *dummy* asignando valores de 0 y 1 a los niveles de esta variable (en este caso, 0 = ningún control; 1 = múltiples controles). Por esto, al asignar 0 y 1 al argumento `newmods` (i.e. `newmods = c(0, 1)`), la función *predijo* la correlación para estudios sin ningún control y para estudios con múltiples controles. También convertí la tabla a un objeto clase `data.frame` (usando la función `as.data.frame()`), creé una nueva variable llamada `controls` que contiene las categorías para las cuales calculé la predicción (`mutate(controls = levels(dat$controls))`, que en este caso corresponden con los niveles de la variable `controls`), y reorganicé el orden de las columnas (`select(7, 1:6)`). El objeto resultante fue asignado a `pred.res.contr`.

#### *Meta-Analytic Scatter Plot* (Gráfico de dispersión meta-analítico)  {#meta-scatter2}

Al igual que en el ejemplo de meta-análisis con moderación de una variable continua (sección \@ref(ex-mod1)), es posible hacer un gráfico de dispersión meta-analítico (*meta-analytic scatter plot*) usando la función `regplot` del paquete `metafor`. Sin embargo, dado que la variable moderadora es categórica, y el modelo generó variables *dummy* asignando valores de 0 y 1 a los niveles de esta variable (0 = ningún control; 1 = múltiples controles), en el ejemplo siguiente agregué esta información a la descripción del eje $X$.

```{r reg-plot2, fig.height = 4.8, fig.cap = "Gráfico de dispersión meta-analítico (*Meta-Analytic Scatter Plot*) básico de [metafor](https://www.metafor-project.org/doku.php) creado con la función \\texttt{regplot}. El tamaño de los puntos es proporcional al peso que recibieron los estudios en el meta-análisis (puntos más grandes para los estudios con más peso, pues tienen un tamaño de muestra mayor y con un menor error estimado). La línea negra representa el efecto previsto en función del predictor (en este caso \\texttt{controls}, controles). La banda gris delimitada por líneas punteadas representa el intervalo de confianza del 95%. Dado que la variable moderadora es categórica, el modelo genera variables *dummy* asignando valores de 0 y 1 a los niveles de esta variable (en este caso, 0 = ningúun control; 1 = múltiples controles), tal y como se describe en el eje *X*. Para una versión más apropiada, ver Figura \\@ref(fig:reg-plot3)."}
regplot(res.contr,
        ylab = "Coeficiente de correlación (z de Fisher)",
        xlab = "Controles (0 = ninguno; 1 = múltiples)")
```

Aunque la Figura \@ref(fig:reg-plot2) presenta información correcta, tal vez no es la más adecuada ni la más clara cuando se trata de representar meta-análisis con moderadores categóricos. Por ejemplo, a pesar de que nuestro moderador es un factor (controles) con dos niveles (ninguno, múltiples), en el eje $X$ se representa como una variable continua con valores entre 0 y 1, que además tiene valores intermedios (0.2, 2.4, 0.6, 0.8) que en nuestro caso no tienen sentido.

Sin embargo, de manera un poco más *artesanal*, es posible crear una versión más adecuada usando, por ejemplo, `ggplot2`. El código siguiente permite crear una figura mucho más adecuada usando este paquete.

```{r eval = FALSE}
# Definir base de datos, así como ejes X (controls) y Y (yi)
ggplot(dat, aes(x = controls, y = yi)) +
  # Agregar puntos para cada estudio, con tamaño y color según tamaño de muestra (ni)
  geom_point(aes(size = ni, color = ni), alpha = 0.5) +
  # Definir escala de colores
  scale_colour_gradient(low = "red", high = "blue") +
  # Definir rango de tamaño de los puntos
  scale_size_continuous(range = c(1, 10)) +
  # Combinar colores y tamaños de puntos en una sola leyenda
  guides(color = guide_legend(), size = guide_legend()) +
  # Traducir etiquetas del eje X en español
  scale_x_discrete(labels = c("none" = "Ninguno", "multiple" = "Múltiples")) +
  # Cambiar títulos de ejes a español
  labs(x = "Controles", y = "Coeficiente de correlación (z de Fisher)") +
  # Agregar barras de error para cada categoría, con base en la predicción de la sección 4.2.1
  geom_errorbar(data = pred.res.contr, mapping = aes(ymin = ci.lb, ymax = ci.ub),
                width = 0.1, color = "black") +
  # Agregar puntos blancos representando la predicción de la sección 4.2.1 para cada categoría
  geom_point(data = pred.res.contr, shape = 21, size = 3, color = "black", fill = "white") +
  # Cambiar título de leyenda
  labs(color = "Tamaño de \nmuestra", size = "Tamaño de \nmuestra") # "\n" para salto de línea
```

El código anterior puede verse algo confuso para quien no haya usado `ggplot2` antes. Por esto, he agregado anotaciones, función por función, para ayudar a su interpretación[^21]. Este código produce la Figura \@ref(fig:reg-plot3).

[^21]: Debido a sus enormes y numerosas posibilidades, `ggplot2` es quizás la opción más poderosa para hacer gráficos estadísticos en R; por esto explicar sus bases supera por mucho el alcance de esta guía. Sin embargo, hay gran cantidad de opciones disponibles en internet. Por ejemplo, puedes leer este [tutorial](https://rpubs.com/anlope10/562981) [@lopezpenarandaTutorialGgplot22019] o ver este [video](https://youtu.be/BUzTAr_QqKs) [@datademiaAprendeGgplot22018]. 

```{r reg-plot3, fig.height = 4, fig.width = 6, echo = FALSE, fig.cap = "Gráfico de dispersión meta-analítico (*Meta-Analytic Scatter Plot*) creado manualmente con [ggplot2](https://ggplot2.tidyverse.org/) para hacer una mejor representación de un moderador categórico. Los puntos de colores representan el coeficiente de correlación en función de la presencia o ausencia de controles. El tamaño de los puntos es proporcional al tamaño de muestra de los estudios inluidos en el meta-análisis (puntos más grandes y azules para los estudios con mayor tamaño de muestra). Los puntos blancos superpuetos representan el efecto estimado para cada categoría, y las barras de error representan los intervalos de confianza del 95%."}
ggplot(dat, aes(x = controls, y = yi)) +
  geom_point(aes(size = ni, color = ni),
             alpha = 0.5) +
  scale_colour_gradient(low = "red",
                        high = "blue") +
  scale_size_continuous(range = c(1, 10)) +
  guides(color = guide_legend(), 
         size = guide_legend()) +
  scale_x_discrete(labels=c("none" = "Ninguno", 
                            "multiple" = "Múltiples")) +
  labs(x = "Controles", 
       y = "Coeficiente de correlación\n(z de Fisher)") +
  geom_errorbar(data = pred.res.contr,
                mapping = aes(ymin = ci.lb, ymax = ci.ub),
                width = 0.1,
                color = "black") +
  geom_point(data = pred.res.contr,
             shape = 21, size = 3,
             color = "black", fill = "white") +
  labs(color = "Tamaño de \nmuestra",
       size = "Tamaño de \nmuestra") +
  theme_minimal()
```

### *Forest plot* y *funnel plot* {#plot-mod2}

Por supuesto, *forest plots* y *funnel plots* pueden ser creados, tal y como describí en la sección \@ref(plot-mod). Para el *forest plots* podríamos usar el siguiente código:

```{r eval = FALSE}
# forest plot con anotaciones adicionales
forest(res.contr,  cex = 0.75, xlim = c(-1.6, 1.6),
       slab = paste(dat$authors, dat$year, sep = ", "),
       showweights = TRUE,
       xlab = "Coeficiente de correlación (z de Fisher)",
       digits = c(2,3L))
# agregar encabezados a las columnas (valores de X y Y deben ser ajustados)
par(cex = 0.8, font = 2)
text(x = -1.6, y = 18, labels = "Autor(es), Año", pos = 4)
text(x = 0, y = 18, labels = "Efecto e IC", pos = 4)
text(x = 1, y = 18, labels = "Peso", pos = 2)
text(x = 1.6, y = 18, labels = "Corr. [95% IC]", pos = 2)
```

Ese código produce la Figura \@ref(fig:for-plot-mod2).

```{r for-plot-mod2, echo = FALSE, warning = FALSE, fig.height = 4.5, fig.cap = "*Forest plot* básico de [metafor](https://www.metafor-project.org/doku.php), para un meta-análisis incluyendo como moderador si los estudios tuvieron en cuenta múltiples controles estadísticos o ninguno. En la ilustración gráfica, además de los efectos originales, se puede ver el efecto de cada estudio estimado cuando se incluye el moderador como polígonos (diamantes) de color gris. Sin embargo, ya no obtenemos una fila al final representando el efecto promediado del meta-análisis, ya que no tenemos un solo efecto."}
# forest plot con anotaciones adicionales
par(mar = c(4,0,0,0))
forest(res.contr,  cex = 0.75, xlim = c(-1.6, 1.6),
       slab = paste(dat$authors, dat$year, sep = ", "),
       showweights = TRUE,
       xlab = "Coeficiente de correlación (z de Fisher)",
       digits = c(2,3L))
# agregar encabezados a las columnas (valores de X y Y deben ser ajustados)
par(cex = 0.8, font = 2)
text(x = -1.6, y = 18, labels = "Autor(es), Año", pos = 4)
text(x = 0, y = 18, labels = "Efecto e IC", pos = 4)
text(x = 1, y = 18, labels = "Peso", pos = 2)
text(x = 1.6, y = 18, labels = "Corr. [95% IC]", pos = 2)
```

De manera similar, podemos obtener un *funnel plot*, pero recuerda que dado que tanto el paquete `weightr` como el paquete `metafor` tienen funciones llamadas `funnel`, debemos pedirle a R explícitamente que use la función `funnel` del paquete `metafor` con el comando `metafor::funnel`. 
Este *funnel plot*, nos mostrará los valores residuales de cada estudio (es decir, qué tanto se alejan del resultado de nuestro meta-análisis, que tiene un valor residual de 0; Fig. \@ref(fig:funnel-plot1)), en vez de los coeficientes de correlación (transformados a $z$ de Fisher).

Adicionalmente no olvides que este **funnel plot** nos mostrará los valores residuales de cada estudio, dado que se trata de un meta-análisis con moderador, tal como lo expliqué en la sección \@ref(plot-mod).

```{r eval = FALSE}
metafor::funnel(res.contr,
                xlab = "Valor residual",
                ylab = "Error estándar")
```

Esto producirá la siguiente Figura \@ref(fig:funnel-plot2).

```{r funnel-plot-mod2, echo = FALSE, fig.height = 3.5, fig.cap = "*Funnel plot* básico de [metafor](https://www.metafor-project.org/doku.php), para un meta-análisis incluyendo como moderador si los estudios tuvieron en cuenta múltiples controles estadísticos o ninguno. La línea punteada vertical representa el efecto meta-analizado que hemos encontrado, así que podemos ver los estudios que encontraron un efecto mayor (derecha de la línea punteada) o menor (izquierda) de éste."}
par(mar = c(4,4,0,1))
metafor::funnel(res.contr,
                xlab = "Valor residual",
                ylab = "Error estándar")
```

### Reporte del meta-análisis con moderador categórico {#reporte-cat}

De nuevo, y tal como lo hicimos en la sección \@ref(heterog-inf) y en la sección  \@ref(reporte-cont), es posible estimar intervalos de confianza de las medidas de heterogeneidad. Para ello, en este caso usamos la función `confint(res.contr)`. El reporte incluyendo esta información podría ser algo como esto:

```{=latex}
\begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
  colback=iacol!5!white,colframe=iacol!75!white,colbacktitle=iacol,
  title=Ejemplo de reporte con moderador categórico,fonttitle=\bfseries, parbox=false,
  boxed title style={size=small,colframe=iacol} ]
  
Dado que hay estudios que controlaron variables y otros que no controlaron ninguna, ajustamos de nuevo el meta-análisis, pero incluyendo este factor como variable moderadora. Al incluir la variable moderadora de controles en el meta-análisis, se encontró que esta tuvo un efecto significativo en la estimación de la correlación entre la escrupulosidad y la adherencia al tratamiento ($Q$(1) = 20.122, $p$ < .0001). De hecho, los estudios que no incorporaron controles estimaron una asociación más alta ($z$ ± $se$ = 0.179 ± 0.021, IC 95\% [0.138, 0.219]) que los estudios que incluyeron múltiples controles ($z$ ± $se$ = 0.017 ± 0.030, IC 95\% [-0.041, 0.075]).

Al incluir esta variable moderadora, se redujo enormemente la heterogeneidad entre los estudios ($\tau^2$ ± $se$ =  0.00 ± 0.0015, IC 95\% [0.0000, 0.0244]; $\tau$ = 0.0002, IC 95\% [0.000, 0.1562]; $Q$(14) = 18.037, $p$ = .205; $I^2$ = 0.00\%, IC 95\% [0.00\%, 80.96\%]), lo que indica que la heterogeneidad original se debía en gran parte a las diferencias metodológicas entre los estudios.  

\end{tcolorbox}
```

Como antes, es importante tener en cuenta que este resultado debería ser reportado después del meta-análisis básico, como se muestra en las secciones \@ref(reporte1) y \@ref(reporte2). Además, al presentar los resultados, te recomiendo citar las tablas y figuras pertinentes que los ilustren, ya que esto puede facilitar la comunicación de resultados complejos. En particular, al reportar los resultados de la moderación, se podría hacer referencia al gráfico de dispersión meta-analítico (sección \@ref(meta-scatter2)). Asimismo, para hablar de la heterogeneidad, se podría citar el *forest plot* y el *funnel plot* correspondientes a este meta-análisis con moderador (sección \@ref(plot-mod2)).

Aunque es importante proporcionar información detallada y exhaustiva en el reporte de los resultados de un meta-análisis, existen diferentes formas de hacerlo. Una recomendación es utilizar como modelos algunos artículos que hayan reportado meta-análisis previamente. Por ejemplo, el trabajo de @hidalgo-fuentesUsoProblematicoInternet2022 es un ejemplo sencillo de cómo reportar un meta-análisis de correlación, tanto con como sin moderador, en español. Por otro lado, los estudios de @salsmanMetaanalyticApproachExamining2015 y @shiCorrelationAdherenceRates2010 presentan reportes de manera distinta pero clara (en inglés).

---------------

# Recomendaciones generales

Hacer un meta-análisis de correlación en R no es necesariamente complejo, pero podría requerir acciones que pueden parecer algo laberínticas. Sin embargo, la confiabilidad del resultado que obtengas, dependerá de varios aspectos. Por esto, la realización de un meta-análisis requiere una comprensión de los métodos estadísticos involucrados, la selección y evaluación de estudios relevantes, y la identificación y consideración de posibles fuentes de heterogeneidad y sesgo.

## Pasos para hacer tu meta-análisis

Primero, debes tener hacer una selección de artículos buena y no sesgada[^22]. Esto mínimamente incluye:

[^22]: Para esto existen varias guías, e incluso la *Declaración PRISMA* que está diseñada para abordar los problemas en la publicación de revisiones sistemáticas y meta-análisis [para información en español, ver e.g., @huttonExtensionDeclaracionPRISMA2016; @pageDeclaracionPRISMA20202021].

    A. Formular la pregunta de investigación clara, con base en teoría

    B. Identificar la bibliografía pertinente

    C. Extraer y consolidar los datos de los estudios (en el caso de correlaciones, cuando menos *r* y *n*)

Con esto deberías ya poder hacer una base de datos para un meta-análisis de correlación simple. Mientras tengas en tu base de datos algún identificador de cada estudio, el tamaño de efecto (*r*) y el tamaño de muestra (*n*), tendrías todo lo necesario para hacerlo. Por ejemplo:

| estudio       | ri   | ni  |
|---------------|------|-----|
| Autores (año) | 0.25 | 425 |
| Autores (año) | 0.65 | 45  |
| Autores (año) | 0.12 | 235 |

Adicionalmente, podrías definir o extraer información de posibles variables moderadoras; por ejemplo, si los estudios se pudieran agrupar según metodología usada o población estudiada, y sospechas que estas variables afectan la correlación encontrada en cada estudio, podrías agregar esta información en una o más variables que podrías usar como moderadores.

Una vez tengas tu base de datos lista, deberías, cuando menos:

1. Hacer el meta-análisis (sin moderador, sección \@ref(meta-cor); con moderador, sección \@ref(met-moderation))

    1.1 Reportar información sobre heterogeneidad (sección \@ref(heterog-inf))
  
    1.2 Hacer un diagnóstico de influencia (sección \@ref(diag-inf); incluir, si es posible y no tienes un número grande de estudios, un meta-análisis combinatorio, sección \@ref(gosh), e idealmente la identificación de estudios que contribuyen a los patrones de heterogeneidad a partir del *GOSH plot*, sección \@ref(fancy-gosh))
  
    1.3 Reportar los resultados usando un *forest plot* (diagrama de bosque; sección \@ref(forest-inf))
  
    1.4 Mostrar la distribución de los resultados en un *funnel plot* (diagrama de embudo; sección \@ref(funnel-inf)) y estimar el sesgo de estudios pequeños (sección \@ref(reg-egger))
  
2. Estimar si hay evidencia de sesgo de publicación (sección \@ref(sesgo-pub))

    2.1 Puedes usar el método *trim and fill* (recorte y relleno; sección \@ref(trim-fill)) o, lo que es mejor, hacer una estimación del modelo de función de peso (sección \@ref(vevea-hedges))

Adicionalmente, podrías:

3. Estimar el poder estadístico del meta-análisis y los estudios meta-analizados (sección \@ref(poder-inf))

## Conclusión

Es fundamental tener en cuenta que el meta-análisis, aunque es una herramienta poderosa, no es una solución mágica para resolver los problemas de investigación. La calidad de los estudios incluidos, la heterogeneidad de los datos y otras limitaciones pueden influir en los resultados y su interpretación. En última instancia, el éxito de un meta-análisis dependerá del rigor y la transparencia del investigador o investigadora en su implementación y en la interpretación de sus resultados.

Sin embargo, si tienes en cuenta y de manera rigurosa las recomendaciones de esta guía, tendrás una base sólida para hacer y comunicar tu meta-análisis de manera transparente, sin importar los resultados obtenidos.

Lamentablemente, en el mundo hispano, especialmente en Latinoamérica, todavía se presentan importantes rezagos en la producción científica, que son resultado en parte del insuficiente apoyo gubernamental y de un sistema global que desfavorece a los países en vías de desarrollo. Sin embargo, también son producto de una falta de confianza en las capacidades propias y de ciertos déficits reales en educación.

Mi objetivo con esta guía es inspirar a investigadores hispanohablantes, ya sean establecidos o en formación, y de diversas disciplinas, a conocer los fundamentos del meta-análisis y su implementación de forma transparente. El fortalecimiento de nuestras habilidades analíticas es un paso fundamental para mejorar nuestra formación y contribuir al conocimiento global.

---------------

# Agradecimientos  {-}

Quiero agradecer especialmente el apoyo de mi gran amigo y colega [Andrés Castellanos-Chacón](https://scholar.google.es/citations?user=od6gf0wAAAAJ&hl=es) por su dedicada revisión de este documento.

---------------

# Referencias {-}

\begin{multicols}{2}
\AtNextBibliography{\footnotesize}
\printbibliography[heading=none]
\normalsize
\end{multicols}

\def\printbibliography{}

---------------

# APÉNDICES {.unnumbered}

# (APPENDIX) Apéndice {-}  

\opensupplement

## *Apéndice A.* Alternativas a `metafor` {.unnumbered #apendice-alt}

A lo largo de esta guía he usado una ruta para hacer meta-análisis que parte de objetos generados con el paquete `metafor`. Sin embargo, existen rutas alternativas para realizar meta-análisis en R. El libro *Doing meta-analysis with R: a hands-on guide* [@harrer2021doing] se acompaña del paquete [`dmetar`](https://dmetar.protectlab.org/index.html) [@Harrer2019dmetar], que contiene y explica opciones para hacer meta-análisis tanto a partir de `metafor`, como a partir de `meta` [@BalduzziMeta2019; @schwarzerMetaAnalysis2015].

Los objetos generados por `meta` al realizar un meta-análisis, permiten usar funciones de `dmetar` para hacer otros análisis como *risk of bias* (riesgo de sesgo), inferencia multi-modelo, detección de *outliers* (valores atípicos), y *p-curve* o curva de valores $p$ [e.g., @simonsohnPCurveEffectSize2014; @simonsohnPcurveWonYour2019], y permiten hacer gráficos distintos. 

Para una guía resumida y concreta de estas opciones (en inglés), recomiendo ver el sitio web del paquete [`dmetar`](http://dmetar.protectlab.org/), y en especial la página [*Get Started*](https://dmetar.protectlab.org/articles/dmetar.html).

### *A1.* Ejemplo: curva de valores $p$ (*p-curve*) {.unnumbered #p-curve}

Como ejemplo puntual, la curva de valores $p$ (en inglés *p-curve*) de nuestro meta-análisis se podría hacer tras ajustar el modelo con la función `metacor` (que hace meta-análisis de correlaciones) del paquete `meta` [@BalduzziMeta2019; @schwarzerMetaAnalysis2015]. Para este ejemplo asignaré el resultado a un objeto que llamaré `res.meta`.

```{r message = FALSE}
library(meta)
res.meta <- metacor(cor = yi, n = ni, 
                    studlab = paste(dat$authors, dat$year, sep = ", "), 
                    data = dat, sm = "ZCOR")
```

El objeto resultante (`res.meta`) puede usarse directamente como argumento a la función `pcurve` del paquete [`dmetar`](http://dmetar.protectlab.org/), para obtener una gráfica con la curva de valores $p$ (*p-curve*).

```{r eval = FALSE}
library(dmetar)
pcurve(res.meta)
```

```{r pcurve-plot, echo = FALSE, out.width = "300px", fig.align = "center", fig.cap = "Curva de valores $p$ (\\textit{p-curve}) creada con \\href{http://dmetar.protectlab.org/}{\\texttt{dmetar}}. Para entender una curva de valores $p$, su relevancia y su interpretación, sugiero leer Simonsohn et al. (2019, 2014), o ver el sitio web del paquete \\texttt{dmetar}: \\href{http://dmetar.protectlab.org/}{http://dmetar.protectlab.org/}."}
knitr::include_graphics("images/pcurve.pdf")
```

La función `pcurve` produce varios elementos relacionados la curva de valores $p$, incluyendo la Figura \@ref(fig:pcurve-plot) que resume este análisis.

Es importante tener en cuenta que, al menos por ahora, el paquete `dmetar` debe instalarse desde GitHub tal como `metameta` (en este caso, con el comando `devtools::install_github("MathiasHarrer/dmetar")`).

## *Apéndice B.* Citas y referencias de paquetes de R {.unnumbered #paquetes-cit}

Por supuesto, los paquetes de R que usemos deben ser citados. Una manera fácil de encontrar la cita que los autores de un paquete quieren que usemos, es la función `citation` en R. Simplemente debemos usar esta función, agregando como argumento el nombre del paquete que queremos citar entre comillas. Esto nos dará la referencia en un formato estándar, así como como en un formato `BibTex` que puede ser usado en documentos \LaTeX, o por muchos gestores de referencia como [Zotero](https://www.zotero.org/). Alternativamente, el formato formato `BibTex` nos permite saber los campos como autores, título y demás, si vamos a crear las citas y referencias manualmente, o incluso crear referencias en diversos estilos usando, por ejemplo, [Online Bibtex Converter](https://asouqi.github.io/bibtex-converter/).

Como ejemplo, en esta guía usé `dplyr` [@WickhamDplyr2021] para manipular los datos. Usando la función `citation`:

```{r eval = FALSE}
citation("dplyr")
```

Obtengo la suguiente información sobre ese paquete:

```{r echo = FALSE}
citation("dplyr")
```

## *Apéndice C.* Paquetes de R usados para crear este documento {.unnumbered #paquetes-list}

```{r echo = FALSE}
pander::pander(sessionInfo(), locale = FALSE)
```
